{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Take-Aways & Highlights from ICML 2022\n",
    "\n",
    "ICML 2022 was my first in-person conference since... 2018. It's been **4 years** since I have walked through a poster-session, gotten beers with researchers I'd only met on twitter before, and sat in a lobby working on items unrelated to the conference. I wanted to take some time after the conference to ruminate on my experience, touch on some papers that I really enjoyed, and give a summary of the debate that took place at the end of the conference (in that order).\n",
    "\n",
    "--- \n",
    "\n",
    "First, I thoroughly enjoyed ICML 2022 - it is the first machine learning conference I have ever attended (in-person) and in general I am very happy I came. Since the start of 2020 my interests have gradually shifted away from NLP-specific problems towards more general ML problems, and I'm excited that I was able to get to know the ML community a bit. I have a few thoughts that I want to jot down while they're still fresh in my mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-person conferences feel marginally about the papers.\n",
    "\n",
    "My experience with online conferences for the last 2 years has primarily consisted of me dedicating a set of days to peruse through underline papers, watch their slideslive videos, watch a couple keynotes and tutorials, and attend maybe one Zoom social event.\n",
    "I found this process _exhausting_, almost chore-like, because my primary goal was to basically consume as much recent research as I could in a few days.\n",
    "It's definitely my own fault - nobody forced me to do this - but I simply wasn't sure what else to do with my conference registration; I did go to online social events but for whatever reason I just got less and less motivated to actually participate. Towards the end of the pandemic I was basically dreading the next conference.\n",
    "\n",
    "This ICML 2022 was an entirely different experience: In a single conference I had more discussions, exchanged more emails, and in general made way more friends than I have in the last 2 years of virtual conferences.\n",
    "In fact, I actually learned _very little_ about the research being presented at the conference and I think that's a good thing -\n",
    "instead of feeling fatigued and drained from trying to consume paper after paper, I left excited about new research ideas from discussions I'd had and motivated to get back to work.\n",
    "More importantly, I left with some new connections that I will hopefully keep in touch with and continue to be able to bounce research ideas off of, something I've been missing sorely for the last 2 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posters are fantastic and we need more of them.\n",
    "\n",
    "This year ICML seemed to have a sincere focus on spotlight talks (I'm obviously not sure if this is normal or not) - the entire day is comprised mainly of 5-minute spotlight talks with a couple 15-minute oral talks spread throughout, and poster sessions are relegated to the end of the day, from about 6:30pm-8:30pm. I cannot stress enough how much I _hate_ this design - most social events, industry or otherwise, start around 6:30-7pm. This means that you have to choose between going to social events or sticking around in a poster session where most presenters are trying to leave so they can go to social events. **Social events and poster sessions are the best part of a conference!** This is not a problem for just one day - all 3 days of the main conference I had to choose between attending poster sessions for more than 30 minutes, or showing up very late to one social event or another. Instead, most of the day was comprised of 5-minute talks that are very hard to follow (because of the time constraint) unless you're already well-tuned to that area's main ideas and jargon, and were often just pre-recorded videos - as a result, I spent most of the day hanging outside in the lobby talking to people while waiting for poster sessions to start. I'm know not the only one who had this issue either: see [Hal](https://twitter.com/haldaume3/status/1549452798877728768), [Claas](https://twitter.com/c_voelcker/status/1550504580412219393), and [Micah](https://twitter.com/micahgoldblum/status/1551568956598673410)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are ICML et al. too big?\n",
    "\n",
    "My prior conference experience with \\*ACL conferences had primed me for an experience where I can approach nearly anybody, any talk, or any poster and usually come away with a good idea of how what was discussed is related to my research ideas or the ideas of the field in general;\n",
    "this was not my experience at ICML.\n",
    "I saw [Dan Roy](https://twitter.com/roydanroy/status/1550492693817597952) mention recently that conferences like NeurIPS and ICML are really 20 conferences wrapped up in 1 -\n",
    "it's hard to disagree.\n",
    "In many ways this is very fun, since it exposes you to a lot of people you wouldn't normally meet and a lot of ideas that you wouldn't normally come across...\n",
    "However, this isn't _that_ helpful for your research.\n",
    "Most people will understand what you do at a very high level, but they won't have a very strong opinion on your most recent ideas or results, and vice versa; it's simply hard to have a very deep conversation about ideas when there is not a lot of common ground.\n",
    "I actually found it _very hard_ to run into people who are working on similar things as me -\n",
    "as a result, while I met a lot of people and enjoyed talking to them about life, traveling, and the weather, _I didn't have a lot of discussions about my research_... and while I certainly liked the people I met and I hope I run into them again soon, a very candid part of me suggests that this is not the primary function of conferences.\n",
    "\n",
    "I think this is exacerbated by the fact that ICML didn't really make much of an effort to gather researchers in similar areas together for the purpose of social interaction; over the pandemic, \\*ACL conferences implemented \"Birds of a Feather\" rooms, designed to get researchers interested in similar topics into a (Zoom) room to have a discussion that was led by a moderator of sorts.\n",
    "It wasn't perfect, and could definitely use some tuning to in-person events, but I think it's a good way to foster meeting people who are at the conference for the same reasons you are.\n",
    "Instead, at ICML people with similar interests as you can be found in the spotlight sessions you attend (during which you're not supposed to talk) and poster sessions (...did I mention the deal with poster sessions?).\n",
    "Most of the events for social purposes are instead offloaded to industry events, which are not filtered by a person's research focus.\n",
    "Your best bet seems to be holding out for a related workshop, where there is much more discussion and reasonable poster sessions.\n",
    "\n",
    "As I mentioned earlier, my research interests switched during Covid and as a result I'm pretty new to the ML community - I definitely did meet people whose research interests overlap and who I'm excited to keep in touch with, and I'm very glad I could go. At the same time I can't help but feel like, especially for newcomers, the experience could be much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Paper Highlights from ICML 2022\n",
    "\n",
    "I want to take some time now to highlight some papers that I enjoyed quite a bit.\n",
    "Generally speaking, these papers fall under the broad category of \"**Understanding Deep Learning**\".\n",
    "This is a category of papers that I'm always looking for at machine learning conferences, and includes papers which highlight some empirical phenomena in neural networks, or papers which connect some empirical findings to theoretical results, in a way that expands our _understanding_ of how these complex models really learn.\n",
    "Additionally, I'm going to avoid highlighting outstanding or spotlight papers, since those are generally more visible already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Multirate Training of Neural Networks](https://proceedings.mlr.press/v162/vlaar22b.html)\n",
    "_Tiffany Vlaar · Benedict Leimkuhler_\n",
    "\n",
    "\n",
    "This paper studies the effects of multi-rate training - training a neural network with different learning rates for different parameters. Namely, they split up a network into _slow_ and _fast_ parameters; the _slow_ parameters are updated only every $k$ steps, whereas the _fast_ parameters are updated every step (pictured below).\n",
    "What is the point of this?\n",
    "The paper opens with a simple example from [Li et al., 2020](https://arxiv.org/pdf/1907.04595.pdf) - a $7\\times7$ patch of noise is added to the center of some CIFAR-10 training samples.\n",
    "It's well-established that, all other hyperparameters fixed, a model with a small learning rate will memorize the patches in the training data at the cost of performance on clean images, but a model with a high learning rate will instead ignore the patches -\n",
    "here the authors show that a _multi-rate_ model can **simultaneously learn both**.\n",
    "This has some important theoretical implications regarding neural network memorization, capacity, and learning rates.\n",
    "\n",
    "![](blog_figs/icml2022/multirate.png)\n",
    "\n",
    "In addition, the authors show that multi-rate training can be used to significantly speed up fine-tuning pre-trained models on downstream tasks without affecting performance, by treating only the last layer as the \"fast\" parameters.\n",
    "Finally, the authors combine multi-rate training with dropout in a novel regularization scheme and show that it can improve performance (rather drastically) over vanilla dropout, suggesting some benefit to making sure the \"dropped\" parameters are updated before being re-activated.\n",
    "Check out the paper for convergence analysis and discussions of techniques like _linear drift_.\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "Simply training different parameters of a model at different rates has important theoretical implications, by changing what a model can simultaneously learn, as well as several practical implications, e.g. efficient transfer learning and a stronger forms of dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Multi-scale Feature Learning Dynamics: Insights for Double Descent](https://proceedings.mlr.press/v162/pezeshki22a.html)\n",
    "_Mohammad Pezeshki · Amartya Mitra · Yoshua Bengio · Guillaume Lajoie_\n",
    "\n",
    "\n",
    "_Epoch double descent_ is the phenomena of _training epochs_ mirroring the double descent curves commonly associated with _model complexity_ in deep learning.\n",
    "This work is interested in studying epoch double descent via linear models which are _complex_ enough to exhibit epoch double descent, but _simple_ enough to be analyzed directly.\n",
    "To do this the authors use a teacher-student model setup to create a training regime where a student model learns to mimic a _noisy_ teacher while being fed _modulated_ inputs (shown below). The modulation matrix $F$ determines which features are more \"accessible\" to the student model during learning, and which are less. \n",
    "\n",
    "![](blog_figs/icml2022/multiscale-feature-2.png)\n",
    "\n",
    "Under this setting, a model trained with SGD has generalization error that can be decomposed into two _macroscopic_ terms, which are composed of either _fast_ features (more accessible) or _slow_ features (less accessible).\n",
    "The _dynamics_ of this model can then be described in terms of these macroscopic variables, where double-descent emerges analytically.\n",
    "This yields the following result, which is pictured below: epoch double descent occurs in models because there exist \"fast\" features and \"slow\" features; Fast features are learned quickly but result in overfitting when solely relied on; slow features are learned much slower but are necessary to generalize well; when they are learned jointly the _epoch_ double-descent curve arises.\n",
    "Finally, the authors rely on a connection between regularization strength and the accessibility of features to show that ResNet18 models trained on CIFAR-10 behave similarly to the student-teachear model setup for different levels of regularization / modulation, corroborating their analytical results to some extent.\n",
    "\n",
    "![](blog_figs/icml2022/multiscale-feature.png)\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "Analysis of a simple linear model which exhibits epoch double descent suggests that epoch double descent occurs due to different features being learned at different rates - as fast features begin to overfit, slow features are learned which further decreases the loss.\n",
    "Moreover, despite the simplicity of the analytical linear model, it can be shown to mirror the effects of modern neural networks with regularization in practice, corroborating the proposed model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [Datamodels: Understanding Predictions with Data and Data with Predictions](https://proceedings.mlr.press/v162/ilyas22a.html)\n",
    "#### _Andrew Ilyas · Sung Min (Sam) Park · Logan Engstrom · Guillaume Leclerc · Aleksander Madry_\n",
    "\n",
    "This paper introduces the concept of a **Datamodel** - a function which models the relationship between the training data of a model and it's output _after training_ on that data.\n",
    "Formally, let $f_{\\mathcal{A}}(x, S)$ be the output of a model on example $x$ after training with algorithm $\\mathcal{A}$ on training set $S$.\n",
    "For a neural network, such a function $f$ is difficult to analyze as it involves optimizing a network with SGD over, say, CIFAR-10.\n",
    "However, this work shows that a simple _surrogate_ function $g(S)$ can be used to approximate $f$ given enough examples of the form $\\{S', f(S')\\}$, where $S'$ is a subset of the total training set $S$.\n",
    "In particular, this work shows that a _linear_ data model i.e. $g_{\\theta_x}(S') = \\mathbf{1}_{S'} \\cdot \\theta_x$ suffices to accurately predict neural network behavior after a sufficient amount of examples - in other words, the output of $f_\\mathcal{A}(x, S')$ can be approximated by a sum of learned weights for each example in $S'$\n",
    "(_this is relatively shocking to me_).\n",
    "\n",
    "Such a simple function of how the training data relates to model outputs has a myriad of applications with respect to model interpretability.\n",
    "For example, the authors show that datamodels can be used to efficiently identify the _minimal subset_ of the total training set for which an SGD-trained model's prediction will flip, exposing brittle examples which rely on a tiny subset of training data for accurate prediction.\n",
    "Another application (shown below) is quickly identifying train-test leakage - train images with high weights in a datamodel are often very similar to the test image and the authors use crowdsourced workers to verify this leakage. \n",
    "There are many more examples of applications using datamodels in the paper and they're all very interesting - check it out!\n",
    "\n",
    "![](blog_figs/icml2022/datamodels.png)\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "Given enough examples of network behavior, a very simple linear function can learn the relationship between individual datapoints in a training set and a neural network's predictions on a fixed test point (after training with SGD). Such a \"datamodel\" is very interpretable and can identify train-test leakage and counterfactual datapoints, among several other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [Synergy and Symmetry in Deep Learning: ...](https://proceedings.mlr.press/v162/xiao22a.html)\n",
    "#### _Lechao Xiao · Jeffrey Pennington_\n",
    "\n",
    "\n",
    "The _curse of dimensionality_ suggests that learning in high-dimensions is infeasible as the number of required training samples grows exponentially with the dimension of the data.\n",
    "Despite this theoretical limitation, modern datasets are efficiently learnable by neural networks, suggesting certain synergies must exist between the inference algorithm $\\mathcal{I}$, the model class $\\mathcal{M}$, and the data $\\mathcal{D}$.\n",
    "To study these synergies the authors examine _symmetries_ between the biases of a learning algorithm ($\\mathcal{M}$ and $\\mathcal{I}$) and the data ($\\mathcal{D}$).\n",
    "They propose a set of rotations that can be applied to images - these rotations (pictured below) destroy many of the spacial properties that encode the semantics of an image but, crucially, maintain their original distances.\n",
    "These transformations should make no difference to a learning algorithm which primarily learns via the distances between inputs (i.e. a dot-product kernel method).\n",
    "\n",
    "![](blog_figs/icml2022/synergy-1.png)\n",
    "\n",
    "The authors also propose a set of learning algorithms $\\{(\\mathcal{M}, \\mathcal{I})\\}$ which encode different biases (Fully-Connected Kernels $\\rightarrow$ Translation Invariance) - they show analytically that the proposed learning algorithms are each invariant to a different subset of the proposed rotations (i.e. they enjoy certain _symmetries_ across data distributions).\n",
    "They empirically validate these symmetries, showing that e.g. global average pooling (GAP) symmetries break after $O(3)^d$ rotations, while feed-forward networks (FCN) are symmetrical across all data distributions (see below).\n",
    "They propose that these symmetries represent _consistencies_ between the learning algorithm and the data distribution - a translation invariant learning algorithm is _consistent_ with the original distribution of CIFAR-10 and thus can more effectively overcome the curse of dimensionality.\n",
    "\n",
    "![](blog_figs/icml2022/synergy-2.png)\n",
    "\n",
    "The authors also show that these symmetries can change _with the amount of training data_, e.g. when a certain data point is reached the consistency between a model class and the data distribution can improve based on how the model learns (a translation-equivariant model learns to behave like a translation-invariant model, for example).\n",
    "They call this phenomena Data Improves Data Efficiency, and demonstrate that it not only occurs in their toy models but also in several SOTA models as well - the power-scaling law for large ResNets can actually change as a function of the data.\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "This work presents a very clever study on the symmetries between inductive biases in the learning algorithm and the data distribution. They find that rotations to the data distribution, which do not affect how feed-forward networks learn, significantly reduces CNN-based model performance; these rotations break certain _symmetries_ between the biases of a CNN and the original data distribution.\n",
    "Finally, they show that the amount of data can effect these symmetries, i.e. the amount of data can change the biases that a model learns with.\n",
    "**This paper took me a while to wrap my head around, but I _really_ like it - it might be my favorite paper from the conference.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Towards understanding how momentum improves generalization in deep learning](https://proceedings.mlr.press/v162/jelassi22a.html)\n",
    "#### _Samy Jelassi · Yuanzhi Li_\n",
    "\n",
    "Why does momentum improve generalization in deep learning?\n",
    "In practice, momentum is almost a given - it is nearly always a good idea to use momentum (often with a heavy coefficient) when training a deep neural network.\n",
    "But in theory, it still is not clear why momentum is _so useful_ for generalization.\n",
    "To study this question, the authors of this paper first put forth a set of experiments showing (a) momentum helps generalization even in _full-batch_ gradient descent, suggesting that momentum does not only help generalization by reducing SGD noise and (b) there exist settings where momentum actually _harms_ performance with deep neural networks.\n",
    "Motivated by these empirical results, the authors attempt to theoretically analyze why momentum helps with deep learning.\n",
    "\n",
    "To do this they construct a binary classification setting where the inputs consist of multiple _patches_ - only one patch per input is predictive of the true label, and it's margin is either large (with high probability) or small (with low probability). The rest of the patches are sampled from noise, and are not predictive of the true label.\n",
    "They model this dataset with a convolutional network with a cubic activation and treat the problem as logistic regression (with classes $-1$ and $1$).\n",
    "The authors prove that, in this setup, a model trained with gradient descent will first learn the examples with large margins, which have the largest gradients - once the gradients of large-margin examples gets small enough, the model will begin to learn the small-margin examples.\n",
    "However, because the gradient from the small margin patches is so small, gradient descent will memorize the examples via their noisy patches rather than continue to learn the half-space predictor.\n",
    "Adding momentum **mitigates** this effect by keeping the gradient correlated with the large-margin examples while learning the small-margin examples - the causes the model to instead memorize the small-margin examples via their predictive patch keeping the model consistent in both phases.\n",
    "\n",
    "\n",
    "#### TL;DR\n",
    "\n",
    "The authors provide a clever, theoretical treatment of momentum in gradient descent - at a high level, their main result suggests that gradient descent first learns high-margin (easy) data, and then memorizes small-margin (difficult) data points. With momentum, the model is forced to memorize small-margin data in a way that is _consistent_ with the previously learned (easy) data, and therefore learns to generalize on small-margin data rather than overfit to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ICML Debate: Science or Engineering?\n",
    "\n",
    "The last social session of the conference featured a debate over the question **\"Will progress towards achieving AI be mostly driven by engineering or science?\"**.\n",
    "I watched it and I have some thoughts - I'm going to summarize the debate first, and then briefly list my thoughts out afterwards.\n",
    "\n",
    "The debate was in the British Parlaimentary style, which essentially boils down to 4 people for each \"side\" who each get to make a 5-7 minute argument.\n",
    "It starts with the \"leader\" of the proposition team, which in this case was the team advocating for engineering, and they are followed by the \"leader\" of the opposition team, which in this case was the team advocating for science.\n",
    "The next in line repeat the pattern, and so on.\n",
    "This means the later speakers have the opportunity to address arguments made during the debate, whereas the opening arguments are largely prepared speeches.\n",
    "The only other thing worth noting: when the member of one party is speaking, a member of another party can raise a \"point of information\" wherein they have about 15 seconds to raise a relevant point or ask a question to the current speaker.\n",
    "The speaker can actually _decline_ the point of interest and continue talking, but I think only one speaker really leveraged this.\n",
    "\n",
    "My goal here is to simply summarize the main points of each speaker, in the order they presented, and hopefully not do such a poor job of it that I get angry emails.\n",
    "\n",
    "#### [Sella Nevo](https://research.google/people/SellaNevo/) (Leader of Proposition)\n",
    "\n",
    "Nevo opens by attempting to lay the groundwork for what science and engineering are -\n",
    "science is about understanding deep truths and letting that understanding drive progress,\n",
    "whereas engineering is about building tools and infrastructure so that we can see what does\n",
    "and doesn't work.\n",
    "Importantly, he concedes that both will contribute to the advancement of AI but ultimately he believes that engineering will be the _major driver_.\n",
    "\n",
    "Nevo next argues that there are many things that engineering allows for in AI that are important for progress but perhaps not the focus of this debate: (a) engineering allows for us to _use_ data - it gives us the technology which generates the data, collects  it, and pre-process it; (b) engineering provides tools that are necessary for ML science, e.g. pytorch, tensorflow, cuda, and even programming languages like python; and (c) engineering allows for the deployment of AI systems in real-world environments.\n",
    "\n",
    "However, all these things aside, Nevo wants to argue that engineering actually _drives_ research in AI as well (or rather, will moving forward).\n",
    "Specifically, he argues that **data is the sole largest driver of AI progress today** -\n",
    "the main bottleneck in AI progress is _no longer conceptual breakthroughs_, but rather data pipelines and resources.\n",
    "As evidence, he cites Open AI's scaling laws, which indicate that for every increase in the order of magnitude of data and compute used we can expect to see a fixed decrease in test loss.\n",
    "Nevo closes by noting that we have no understanding of how these models work - theory lags far behind empirical results, and these empirical results were achieved by first _building the models_ (using engineering) to see what works.\n",
    "\n",
    "\n",
    "#### [Kyunghyun Cho](https://kyunghyuncho.me/) (Leader of Opposition)\n",
    "\n",
    "\n",
    "Dr. Cho opens the opposition by claiming that the recent progress in large-scale models such as GPT-3 would appear to paint the picture that impressive feats of engineering are the driving force behind progress in AI. However, \n",
    "/while massive engineering effort is the driving force behind models like GPT-3 et al., massive engineering will not be the dominant force behind progress _towards achieving AI_.\n",
    "\n",
    "To illustrate his point, Dr. Cho highlights several examples of how science has been critical to leading us where we are now.\n",
    "As a first example, he points out out that LLMs and other self-supervised models are based on information theoretic principles which were laid down by Claud Shannon all the way back in the 1950s.\n",
    "While engineering has certainly pushed this bounds of what we can achieve with this idea, it nevertheless was born from scientific theory and would not even exist without it.\n",
    "Dr. Cho moves on to note that several \"tricks\" of deep learning, without which progress in deep learning would have slowed significantly, are also rooted in science. Non-saturating non-linearities such as ReLUs, which are essential to training neural networks, were initially derived from theory on Restricted Boltzmann Machines.\n",
    "Shortcut connections started from mathematical analysis back in the 90's and then empirical analysis up until we derived LSTMs and Residual connections to solve the problem of propagating gradients and long-range credit assignment.\n",
    "\n",
    "Dr. Cho ends his speech by noting that, although it may look like engineering (and scale) is pushing the field forward, **eventually we will hit a crossroads - only science can tell us which direction to go next**.\n",
    "\n",
    ">_Professor Cho wrote a post capturing his speech and thoughts [here.](https://kyunghyuncho.me/my-opening-statement-at-the-icml-2022-debate/)_\n",
    "\n",
    "#### [François Charton](https://twitter.com/f_charton?lang=en) (Deputy Leader of the Proposition)\n",
    "\n",
    "Charton responds to Dr. Cho, noting that recent advancements in AI necessarily _use_ science - but they are not _driven_ by science.\n",
    "As an example, he notes backprop - a very simple scientific idea - brought neural networks back into popularity in the 80s precisely because it allowed us to try neural networks _on the computers that were available at the time_.\n",
    "More recently, GPUs and libraries allowing us to leverage GPUs for tensor calculations were essential to making progress in ML - were it not for GPUs and Cuda we would still be stuck trying to train deep neural networks.\n",
    "Charton additionally notes that many of the tricks that Dr. Cho noted on, e.g. dropout, batchnorm, relus, residual connections etc. were initially proposed with hypothetical explanations that do not hold anymore -\n",
    "nevertheless, we still use these tricks because _they work_, regardless of their theoretical validity.\n",
    "\n",
    "Finally, Charton argues that in our field, it's engineering that first affects change and makes experiments possible, and science which comes in later to make discoveries and explanations - the latest example of this being humongous language models and their emergent properties. Will this pattern continue in the future? Charton argues that our community has become shaped by engineering practices (e.g. reproducible code, reporting hyperparameter and model details, and the way we recuit people) -\n",
    "because of this he believes **we are primarily a community of engineers** and so we will be driven by engineering innovations first.\n",
    "\n",
    "#### [Ida Momennejad](https://www.momen-nejad.org/) (Deputy Leader of the Opposition)\n",
    "\n",
    "Dr. Momennejad wishes to make 3 major points during her speech.\n",
    "Her first point is that the carbon footprint of training e.g. GPT-3 is _massive_ - scaling up our current solution to achieve AGI is not a sustainable goal.\n",
    "The cost of attempting to achieve AI by scaling our current solutions might simply _not be worth_ the impact it has on our planet.\n",
    "Her second point is that recent studies of neural networks, e.g. recent findings around pruning, confirms that our current solutions are significantly inneficient. Dr. Momennejad argues that we should be guiding our search for intelligence with the various _efficient_ sources of intelligence that we see around us. The brain, multi-agent ecosystems, etc. all provide examples of intelligence that is not only efficient but has learned to exist in a community. Engineering, within the current paradigm of large models and tons of data, is not enough to achieve these types of intelligence.\n",
    "\n",
    "The final point is that most of engineering is still doing science - if, instead of randomly trying things, you formulate a hypothesis and test it empirically you are doing science. However, **engineering may be doing \"_amateur science_\" by ignoring domain experts and reframing the question** of what the models are learning or achieving.\n",
    "\n",
    ">_At the end, Nevo raises a point of interest noting that by Ida's thinking, you are either doing science or randomly trying things. Is engineering really just \"randomly trying things\"?  \n",
    "Ida claims that when engineering  does not randomly try things, it is guided by hypotheses which are rooted in science. \"The objective functions of engineering are provided by either science, or the social / moral needs it's trying to serve.\"_\n",
    "\n",
    "\n",
    "#### [Maya Gupta](https://www.mayagupta.org/) (Member of Proposition)\n",
    "\n",
    "Dr. Gupta starts her speech by directly addresing some of Dr. Momennejad's prior points.\n",
    "She notes that current SOTA models are indeed very expensive to train, but that improving efficiency and reducing carbon emissions is largely an _engineering problem_.\n",
    "Dr. Gupta also takes issue with idea that we should model systems after the brain: \"we built planes instead of artificial birds\".\n",
    "Finally, Dr. Gupta argues that science is not necessarily needed to figure out how to work with humans, citing recent technology in Google, Netflix, and Amazon as evidence that AI can interact with humans without needing more science.\n",
    "\n",
    ">_Here Dr. Momennejad raises a point of interest, arguing that she did not suggest we should copy the brain but instead should be inspired by it's efficiency._\n",
    "\n",
    "Dr. Gupta then moves into her main points, which is that the paradigm we find ourselves in is one of _flexible models_ and _lots of data_;\n",
    "The more flexible the model, the more we can capture the complexity of reality.\n",
    "This paradigm has been very successful, but **we require more engineering solutions to handle extremely flexibly models and vast quantities of data**.\n",
    "In fact, she notes that most recent progress has actually benefited from _less science_ -\n",
    "As evidence, she cites\n",
    "[that](https://twitter.com/yoavgo/status/1431287117553053700)\n",
    "[one](https://twitter.com/TaliaRinger/status/1494699952207011844)\n",
    "[quote:](https://twitter.com/j6mes/status/1141288879129059328)\n",
    "\"Every time I fire a linguist my speech recognition system gets better\".\n",
    "Dr. Gupta also argues that we need more accessibility to create a larger talent pool, and this accessibility is supported through efforts like standarization of benchmarks and open source software - \n",
    "tools that are driven by engineering efforts.\n",
    "\n",
    ">_Dr. Cho raises a bit of a cheeky POI: \"If the number of people equates to progress, why is the U.S. so bad at soccer?\"_\n",
    "\n",
    "Dr. Gupta closes by mentioning that, historically, technology is driven by engineering first:\n",
    "We had catapults before we understood newtonian mechanics;\n",
    "We had good smallpox vaccines before we understood how antibodies work;\n",
    "And we had good plant breeding before we understood genetics.\n",
    "Thus, she argues that **we may still have many years of progress in AI driven by engineering before science is needed**. \n",
    "\n",
    "\n",
    "#### [Pulkit Agrawal](https://people.csail.mit.edu/pulkitag/) (Member of Opposition)\n",
    "\n",
    "One of Dr. Agrawal's main points during his speech is that the field of ML is dominated by _empirical science_.\n",
    "For instance, even though we can't prove that SGD converges to a good solution for neural networks, through millions of scientific experiments we know that it often works well.\n",
    "Moreover, when trying to train a neural network our decisions are often based on prior scientific results - if our loss doesn't go down we assume there is something wrong with the optimizer not the model class, because neural networks are universal function approximators.\n",
    "Additionally, much of the work done here at ICML is of the type that compares on method against another - if the proposed method is better than the previous method, then we publish a paper on it.\n",
    "Each of these are instances of the _scientific method_, not feats of engineering, and they drive the field still today.\n",
    "\n",
    "Dr. Agrawal's second point shifts to topics such as safety, transparency, and fairness - we want to impose these concepts on our systems to make sure that they are useful to us.\n",
    "To develop these concepts we need to study humans and their psyches -\n",
    "Therefore, **to deploy AI systems in the world we need to rely on the _science of humans_**.\n",
    "Finally, he notes that a similar problem arises when we try to define AI - we still don't have a single definition or benchmark of intelligence that is agreed upon. Defining what intelligence is and how to measure it requires more study of humans, of human intelligence, and of other types of intelligence in our world - i.e. science.\n",
    "\n",
    "\n",
    "#### Sella Nevo again, on behalf of [Been Kim](https://beenkim.github.io/) (Proposition Whip)\n",
    "\n",
    "Nevo closes the Proposition position by reviewing the arguments of the Opposition, and in particular raising 3 key points that the proposition disagrees with:\n",
    "- The first is that engineering has no goal without science - Nevo argues that generations of engineers have been motivated by the desire to build something _useful_.\n",
    "- The second is that we need new scientific ideas to drive progress today. Nevo argues that because we are trying to build flexible models which come up with innovations _on their own_, it's not guaranteed that we will ever need to return to novel scientific ideas.\n",
    "- The final point is that major breakthroughs have been driven by science so far. In response, he cites OpenAI who, in their GPT-3 paper, explicitly claim that the humongous size is the sole driver of it's benefits over prior models.\n",
    "\n",
    "Finally, Nevo claims that **the true debate lies in the question of whether or not we _need to understand something_ to make progress on it**.\n",
    "He asks how many modern AI advances are driven by such an understanding versus someone trying something new and showing that they have improved the model in some way.\n",
    "Nevo claims that this type of exploration, empirical science relying on many experiments and benchmarks, is driven by how many different configurations and parameters we can control and is therefore driven much more by engineering than scientific progress or insight.\n",
    "\n",
    "#### [Sujoy Ganguly](https://blog.unity.com/author/cap-sujoy-ganguly) (Opposition Whip)\n",
    "\n",
    "Similar to the Proposition whip, Dr. Ganguly will go through and refute a set of points that largely summarize the position of the proposition, the first of which is on the democratization of AI code and resources - Dr. Ganguly agrees that this is both an extremely important endeavor as well as one that is led by engineering.\n",
    "But the reason it is so important is because it allows many people to be able to compare their ideas to other ideas.\n",
    "He stresses that running experiments, making observations, and coming up with testable hypotheses is all science, and this is what the bulk of work in machine learning is.\n",
    "\n",
    ">_Nevo raises a POI: \"Could you clarify what, if any, work related to AI would not be science\"  \n",
    "Dr. Ganguly's response: \"Creating tensorflow or pytorch is not science - running an experiment with them is science\"_\n",
    "\n",
    "The next major disputed point is that new theoretical ideas are not necessary to progress AI further.\n",
    "Dr. Ganguly points out that theory is not the same thing as science -\n",
    "He references thermodynamics, which generated laws based on empirical observations first, and wasn't unified under a  theory until some time later.\n",
    "However, he also refutes that we don't have any need for new ideas, citing contrastive learning and neural fields as two recent innovations which are theoretically motivated and have driven recent progress in computer vision.\n",
    "\n",
    ">_Nevo raises another POI: \"Can you name a single scientific innovation that has contributed to AI capabilities as much  as increases in computation and data have improved LLMs capabilities in recent years?\"  \n",
    "Dr. Gupta responds: \"Neural networks\"  \n",
    "Nevo: \"In recent years?\"  \n",
    "Dr. Gupta: \"Neural fields came through a realization that we could model things continuously, and relied on field theory. This was a scientific idea and required new scientific thinking.\"_\n",
    "\n",
    "Dr. Ganguly ends by reiterating two key points of the Opposition:\n",
    "**Scientific methodologies determine _how to compare and contrast_ different methods, which sets the path for how we move forward.**\n",
    "Finally, as Dr. Agrawal also pointed out:\n",
    "We need concepts of fairness, transparency, and accountability to be embedded in our models in order to deploy them in the real world.\n",
    "These ideas won't come through scale but rather come from careful & thoughtful experiments, observations, and metrics - all of which are science.\n",
    "\n",
    "#### The results\n",
    "\n",
    "![](blog_figs/icml2022/debate-poll.png)\n",
    "\n",
    "A pre- and post-debate poll was run on the current audience, whose results are posted above.\n",
    "Based on these results, it was declared that the opposition \"won\" the debate, since the margin between the pre-debate poll increased.\n",
    "However, it's pretty clear that more people actually voted in total at the end so it's up in the air as to whether any minds were changed or if a bunch of scientists just entered the room late.\n",
    "\n",
    "#### My Thoughts\n",
    "\n",
    "Sella Nevo opened the debate by trying to set the framework for what it means for science to make progress on AI, and what it means for engineering to make progress on AI.\n",
    "His framing is that science drives progress through understanding first and then leveraging that understanding to make progress, whereas engineering drives progress by enabling testing and hill-climbing.\n",
    "If our goal was to put people in space, science might make progress by trying to understand what challenges we face and why, whereas engineering may try to make progress by building different types of rockets and seeing which ones could make it out of the atmosphere.\n",
    "I think it's clear that science and engineering would both be necessary for almost any endeavour; the question is which (understanding or testing) would provide the most innovation.\n",
    "\n",
    "The crux of this debate was perhaps meant to be over whether or not the current paradigm of large models and massive data is the key to progress in AI;\n",
    "will more progress come from scaling up models and data, or will more progress come from novel understandings of how these models work and their shortcomings.\n",
    "However, the opposition brought up two very important reasons that this question is already something of a non-starter.\n",
    "\n",
    "The first reason, highlighted by Dr. Momennejad and Dr. Ganguly, is that the _goal_ of progress in AI, and indeed what metrics are meaningful to hill-climb on, are still unknown.\n",
    "If engineering is about building infrastructure so we can see what works and what doesn't, we need a notion of what it means for something to \"work\".\n",
    "Sending people to space is a relatively well-defined goal, with fairly clear metrics of success.\n",
    "However, there is little agreement on what even constitutes intelligence, much less how we can measure it;\n",
    "in fact, the only thing there seems to be high agreement on is that our current benchmarks are not sufficient.\n",
    "It seems pretty clear that the science of intelligence&mdash;human or otherwise&mdash;is still necessary to make progress on AI. \n",
    "\n",
    "The second reason, raised by Dr. Momennejad and Agrawal, is that _how we make progress_ matters.\n",
    "It may be that an engineering driven or a science driven approach would both succeed equally well at getting people to space;\n",
    "however, an engineering driven approach may result more rocket launches, which could have a serious impact on the climate.\n",
    "AI progress faces very similar concerns, and not just with the environment.\n",
    "What are the risks of collecting so much data? Are there privacy concerns? Does the data _actually_ reflect a global distribution? Does the data represent values that we want to instill in our models? How can we ensure that these models will benefit _everyone_? These are real concerns raised by scientists, e.g. [Bender et al.](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922), and answering these questions requires science (and not just computer science).\n",
    "\n",
    "Finally we get to the question of scale: will progress be driven _more_ by understanding _how_ and _why_ these models work and don't work, or from iterating on the same core idea with ever increasing scale?\n",
    "The Proposition argued that we don't need to understand these models to make large strides in progress; Dr. Gupta raised the point that we had catapults before Newtonian mechanics, and Nevo argued that with increased scale the models would learn necessary innovations _themselves_.\n",
    "The Opposition instead argued that science is needed at the \"turning points\" of AI and, as Dr. Cho says, we will eventually hit a cross-roads.\n",
    "Science is the steering wheel and engineering is the gas:\n",
    "the proposition is betting that we are on a largely straight road with slight bends, and the opposition is betting that there are some very important turns in the future.\n",
    "My feeling is that, regardless of which is right, engineering is pressing the petal down pretty hard right now.\n",
    "We owe it to ourselves (and others) to slow down;\n",
    "if we're going too fast we may miss some pretty important turns, and lose out on progress in the long run... but even worse, we could be unprepared for a sharp turn and crash, and that could be pretty disastrous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
