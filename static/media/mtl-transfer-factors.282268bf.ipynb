{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can Optimization Explain Multi-Task Transfer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Intro Note**: _This post is about a paper that I recently wrote, and I wanted to give a bit of backstory here.\n",
    "In late 2020, I had been studying how to improve multilingual models by applying specialized multi-task optimization methods to training, but I wasn't finding any success with them; using the standard uniform average gradient across all tasks would often work just as well, if not better, than these special optimizers. I ended up dropping this thread, but during the process I got pretty familiar with these methods (I implemented most of them and I combed through their proofs and results)._\n",
    "\n",
    "_Later on, maybe closer to 2022, I was working on multi-task learning for ensembles, and I came across the surprising result that reducing the batch-size beyond what I thought was reasonable for multi-task models resulted in much stronger generalization for pretty much all tasks (way below the optimal batch-size for single-task learning over the same tasks).\n",
    "My advisor, Nick, suggested I look into the linear scaling rule as a potential explanation for this, and this turned into an investigation into how the linear scaling rule evolved under multi-task learning.\n",
    "We wrote up this investigation for the OPT workshop at NeurIPS, and this paper was basically the follow up of that work._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A misalignment between motivation and cause?\n",
    "\n",
    "I want to be careful here, because I don't want it to sound like I'm ragging too much on prior work... but there have been quite a few multi-task optimization methods proposed in prior work and most of them are motivated or validated by either:\n",
    "- Proving that the proposed optimization method converges faster than \n",
    "- Showing that the method avoids poor local minima that other optimizers do not.\n",
    "- Both of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where does transfer \"happen\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
