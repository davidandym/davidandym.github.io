{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My research interests over time (or, my PhDescent)\n",
    "_Posted: 6/15/2019 Edited: 10/9/2022_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be interesting to retroactively examine the topics that interested me the most over the course of my PhD. Below are very broad paragraphs summarizing what was capturing my interest over a given semester (in reverse chronological order) along with some commentary starting from when I first started down the path to starting my PhD in the fall of 2017. The **bold** terms are the TL;DR for each paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025\n",
    "\n",
    "### Spring\n",
    "\n",
    "2025 began with me starting as a new Applied Scientist at Amazon. I moved to Seattle temporarily during this time to meet my new team and get acclimated to working with a team, as opposed to by myself as I had been for the last 6 years. This was a pretty intense time for me, as I would end up traveling back and forth from Seattle to NYC a lot, while working a lot harder than I had in a while (it turns out that having a team that is relying on you is very motivating, haha).\n",
    "\n",
    "Overall, I think this was a period of major growth for me. It was really important for me to break out of my rut and just move on to something new. Over the past 5 years, since Covid, I could feel my ability to push through difficulties at work declining. Working remotely and independently, coupled with an every growing sense of imposter syndrome and feelings of inferiority, had led to a very fragile relationship between myself and my research. I would oscillate between being extremely excited about an idea or project and despairing whenever the experiments didn't turn out exactly the way I had imagined. This despair would, in turn, slow me down tremendously as I began to second guess everything I had been working on up to that point.\n",
    "\n",
    "Working on a team at Amazon that moves pretty fast has forced me to reorient my feelings on research. I think the old me would have been frustrated with the speed at which we work, and the fact that we were probably missing things and not striving for \"perfect\" research. However, the current me can at least appreciate that progress can be made even when we don't understand everything, and even when the goals and metrics are not perfect. And through that, I think I've come to appreciate that, while we should always be super critical of our work, our findings, our experiments, etc. we can't be so critical that we never make progress. Of course that seems obvious, but over the last few years of my PhD I think I was so bogged down by my want for a \"perfect\" thesis that I never produced an actual thesis. I'll have more to say on that in another post I think.\n",
    "\n",
    "As far as research goes - I won't say too much about what I'm doing at Amazon on this post, because I want the stuff I write here to be focused on my PhD journey. But, this \"semester\" was basically me trying to wrap up the final project of my thesis and then push out a complete thesis. That chapter is about **understanding when multi-task learning changes the function we learn** and when it doesn't. We study factors like optimization, model capacity, and importantly, we measure behavioral changes in more ways than just in-distribution test performance, which is what I had been focused on (i.e. negative transfer) for so long. I also spent some time helping out on [Sophia's](https://sophia-hager.github.io/) project about teaching language models to verbalize calibrated confidence, although if we're being honest I didn't really help out on it all that much (I probably don't deserve authorship but Sophia is very nice).\n",
    "\n",
    "Oh, and I got married this January (:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024\n",
    "\n",
    "### Fall\n",
    "\n",
    "Biggest focus this semester? **Job applications** - my goodness, what a crazy past few months that was. I ended up accepting a position as a research scientist at Amazon, even though I hadn't yet defended my PhD.\n",
    "\n",
    "\n",
    "### Spring\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023\n",
    "\n",
    "### Fall\n",
    "\n",
    "Returning from my internship, I was determined to complete the two threads that I had been working on prior to leaving.\n",
    "One of those threads was trying to resolve a conflict (at least, it was a conflict in my mind) between this common conception of negative transfer arising during multi-task training, and the growing practice of instruction-tuning which would fine-tuned a language model on hundreds of tasks simultaneously.\n",
    "I was mainly interested in two points, namely (a) whether some of the benefits of instruction-tuning on unseen task performance could be explained via **positive transfer** from the multi-task learning portion of instruction-tuning, and (b) if not (i.e. if the multi-task learning portion was harming transfer to unseen tasks) whether we could improve task performance by trying to address multi-task conflict in instruction-tuning.\n",
    "I think that I'm pretty unsatisfied with the paper though, it felt a bit... \"meh\". I think it was rushed and only scratched the surface of these questions. This paper was published as an ACL Findings paper in 2024.\n",
    "\n",
    "Part of the reason why I think that paper felt a bit unfinished is because I was simultaneously trying to make sense of the other thread I had been working on, which was **understanding where multi-task learning comes, from the perspective of optimization**. If you have been reading this chronologically, you know that this project originally started as an attempt to improve multi-task transfer in training by addressing multi-task gradient \"conflict\" but, more specifically, by trying to understand why gradient conflict impacted task performance.\n",
    "Unfortunately, it was turning out to be _really, really hard_ to answer that question and this was causing me no small amount of distress.\n",
    "On the one hand, I was reading paper after paper that were producing good results by addressing some aspect of gradient conflict or another; on the other hand, I couldn't see any concrete evidence as to why this should help or work in principle and (to be blunt) I was also having a lot of trouble replicating the results from these papers. If I had had a bit more confidence in myself, at this point at might have just written a paper saying so, but I was so sure at this point that to have a paper worth publishing I needed to have some positive result to talk about.\n",
    "\n",
    "I think mentally, I wasn't doing so great this semester. I was just starting to feel a bit frustrated with my progress, especially as most of my peers began to wrap up their PhDs, defend, and move on to the next phases of their career at this time. It was hard not to look around and feel as if I was massively behind and that I was doing something wrong or was otherwise somehow incapable (i.e. not smart enough) to do this.\n",
    "\n",
    "\n",
    "### Summer\n",
    "\n",
    "This summer I did a research internship at Netflix, in the Bay Area.\n",
    "This was a pretty exciting and fun experience, it was really nice to step outside of my ongoing research and focus on a specific problem for a few months.\n",
    "My project was still focused on multi-task learning, but it was more engineering focused in the sense that we were really just trying to leverage multi-task learning for a particular problem.\n",
    "I spent some time exploring other aspects of the problem less related to MTL, but I'm honestly not sure what I'm at liberty to talk about, haha.\n",
    "\n",
    "I had a lot of fun in the Bay Area; the weather is absolutely incredible (it is not exaggerated) and having a car meant that I got to train BJJ again for the first time since Covid.\n",
    "Also working in an office (especially a _really nice_ office) was amazing and I was dreading going back to working from home every day.\n",
    "\n",
    "\n",
    "### Spring\n",
    "\n",
    "The spring was largely a continutation of what I was working on in the Fall, but with less conference and holiday time.\n",
    "My work on the temperature of SGD evolved into studying the **implicit biases of SGD and how multi-task learning affects them**; in particular, there was evidence that multi-task learning overfits _more_ than single-task learning, which was surprising and counter-intuitive to a lot of multi-task optimization methods, and I was looking to explain that result.\n",
    "I nearly finished our paper on how **task-conflict impacts in-context learning through instruction tuning**, but the paper wasn't quite finished by the time I left for my internship in the Summer.\n",
    "\n",
    "This spring semester was frustrating, to be completely honest. I felt like I was actually working _really_ hard this semester and yet, when all was said and done, I really didn't have a lot to show for it.\n",
    "Both papers that I was working on just didn't really come together, and in retrospect I attribute this to my dimishing self-confidence;\n",
    "it became so, so hard for me to put a result into the paper without having immediate doubts that the experiments had been done correctly and rigorously.\n",
    "Perhaps this would have been different if I hadn't been so focused on writing analysis papers.\n",
    "I also proposed to Julia at the end of May and she said yes, so I'm engaged!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022\n",
    "\n",
    "### Fall\n",
    "\n",
    "This fall my focus was largely on two things: starting up a project in which we study **how task conflict and negative transfer in instruction tuning affect in-context learning** and trying, desparately, to reach a conclusion to the **temperature of SGD in multi-task learning** paper.\n",
    "The latter ended up becoming a rather weak workshop submission at NeurIPS, which was fairly frustrating because it felt like a very shallow framing of our research in this direction; I was determined to keep working on this paper until I felt like it fully captured the depth my findings of the last year.\n",
    "I was additionally still working on the role of multi-task learning in underspecification: during this semester I turned to **shortcut learning** as a framework to study some potential benefits of multi-task learning. I was interested in whether or not multi-task conflict could, in certain cases, be indicative of the model being prevented from learning single-task shortcuts.\n",
    "\n",
    "This fall I got to attend NeurIPS, my third ever in-person conference.\n",
    "NeurIPS was easily the biggest, most energetic conference I had ever been to, but it was a somewhat frustrating experience because I kind of imposter-syndromed myself out of connecting with other researchers.\n",
    "Going in, I knew pretty much no-one else at the conference and, despite being _really_ excited aobut the work being presented, I failed to connect with others working on similar problems because I lacked the confidence to talk about my own ideas and opinions.\n",
    "A frustrating experience indeed, looking back on it.\n",
    "I was also supposed to attend EMNLP in Abu Dhabi but I couldn't fly because I ran into the 6-month passport expiration date issue, which I hadn't been aware of at the time (oops!).\n",
    "\n",
    "### Summer\n",
    "\n",
    "This summer passed by very quickly for me, but I accomplished 2 key things!\n",
    "We ran a paper from (more or less) start to finish in a couple months and submitted it to the upcoming EMNLP. The paper was a move towards an earlier question I'd been ruminating on, which was about **how task-specification might affect multi-task conflict**.\n",
    "This paper was a first step towards this question, focusing on how the switch from \"shared-encoder, task-specific classifiers\" to \"shared encoder-decoder with task prompts\" affected things like multi-task conflict and negative transfer in multi-task settings.\n",
    "I also (finally) attempted my GBO, which was titled **\"The Role of Conflict in Multi-Task Learning\"**. I unconditionally passed, which means I got the go-ahead to start working on the different areas I identified as chapters of my eventual thesis.\n",
    "\n",
    "I also attended ICML this summer, and I wrote a blog post about it! It was my first time attending an in-person conference since Fall of 2018 and it was so great to finally be meeting people and talking about research face-to-face.\n",
    "\n",
    "### Spring\n",
    "\n",
    "Most of this semester was spent working on the paper I \"finished\" last semester; namely, we had noticed a consistent trend with respect to the optimal learning rate for a multi-task model as the number of tasks increases.\n",
    "We connected this to the **temperature of SGD & generalization**, which launched an investigation into how multi-task objectives can effect gradient noise, and how that can yield negative transfer. An earlier version of this paper got rejected from AISTATS last semester, and I think it got rejected from ICML this semester. However, I remained pretty confident that the work was saying some interesting stuff for multi-task learning, so I wasn't giving up on this analysis yet. Additionally, I kept playing around with some ideas surrounding how we could show that **multi-task learning can mitigate underspecification**. However, this started with experiments around NLI and a random assortment of auxilliary tasks, with very mixed results. I think this direction needs a more principled approach, which is what I'll turn to next semester.\n",
    "\n",
    "Since moving, my mental health has been doing a lot better. I think this is likely less about NYC itself, and more about the fact that things were opening up again and I was starting to interact with people in person on a more regular basis... New York is very fun, though.\n",
    "The one thing that remained hard this semester was the distance between me and my labmates.\n",
    "I've met a lot of great people here in NYC but few of them really work on similar topics to me; I was definitely missing daily conversations about specific research ideas and dunking on the latest trends in the field (although few of my labmates actually work on similar topics as me, anyways).\n",
    "Other than that I was feeling really good about my PhD; I felt excited about the direction my research was moving in and was generally pretty motivated.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021\n",
    "\n",
    "### Fall\n",
    "\n",
    "For the first time since early 2020 I finished a paper! As of the writing of this section it hasn't been published, but I like the paper quite a bit and I'm excited about it's direction. It is broadly focused on some **empirical properties of optimization and multi-task learning**, and in particular **patterns that arise as the number of tasks changes**. I had also begun a project that was interested in examining how the **way in which we specify tasks affects inter-task conflict**, which was a paper more focused on the NLP aspect of multi-task learning and the rising trend of prompt-based task-specification. Finally, I was still interested in how **multi-task learning affects solution diversity** - However, I had realized that the answer to this question was not so straight forward, and depended on certain optimization properties that led to the first paper I discussed above.\n",
    "\n",
    "At the beginning of this semester I moved with my partner to New York City and I started to rent out a coworking space. I was finally no longer working from home and it had a serious impact on my productivity; all of a sudden I was getting up early, sticking to a consistent schedule, feeling motivated and excited about research. Because of the vaccine mandate in NYC, I started to feel safer about going to the gym, eating at restaurants, etc. and that had such a drastic change on my mentality as well. It was almost like there had been this very low, confining cieling over my emotions, keeping me from feeling much more than a baseline level of excitement, and it was suddenly just gone. Of course, as I write this things are beginning to close back down because of the new variant, omicron, sweeping through the city - but I have hope that this new variant will pass quickly, and things will return to normal soon. I hope these aren't some \"famous last words\"...\n",
    "\n",
    "### Spring\n",
    "\n",
    "This spring I spent a large amount of time looking at **ensemble diversity with respect to multi-task models**. I was interested in answering the question of whether or not multi-task learning could have a negative effect on ensembling because of it's potential to reduce diversity between solutions. Related to this, I was also working on examining whether or not we could use **multi-task learning to mitigate underspecification in NLP**, to find models that were more reliably robust and agreed with humans on more challenging language understanding tasks.\n",
    "Finally, I was also attempting to quantify some **local properties of multi-task loss landscapes**. In particular, I was wondering if we could see consistent behaviors regarding how the local minima of multi-task loss landscapes related to nearby local minima of the single-task loss landscapes, and if there were any quantifiable differences between those solutions and solutions found using only a single-task objective.\n",
    "\n",
    "However, on a personal level, this semester was particularly rough for me. My productivity felt almost non-existent. I failed to reach 3 (I think?) conference deadlines that I set as goals, for various reasons (either not getting the results I wanted, or getting sudden doubts about the overall idea). I began to really feel the weight of having published only one first-author paper since starting my PhD, and a lot of doubt as to whether or not I could actually do good research began to creep in.\n",
    "Additionally, the total lack of regular communication and technical discussions with my peers was killer, especially as I began to work my way into research areas that I was far from an expert in and, I felt, required way more rigor and technical ability than I possessed.\n",
    "I have to express a lot of thanks to Nick and Mark for constantly attempting to keep me sane and give me perspective, especially Nick who really helped me regain some belief in myself that I was capable of making important contributions with the work I was doing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020\n",
    "\n",
    "### Fall\n",
    "\n",
    "This semester my interest in multi-task learning began to join with my interest in ensembles and bayesian deep learning, resulting in a deep interest in the effects of **multi-task learning on loss landscapes**. This launched a project which has not yet culminated into anything substantial, but is certainly at the forefront of my interests. Additionally, I developed a strong interest in **deep multi-task learning optimization methods**, in particular those that modify the gradients of models, or methods that allow you to keep the hypothesis class constant between single-task and multi-task settings. A number of methods of this flavor were recently presented at ACL and Neurips 2020, and I wondered if my empirical exploration of loss-landscapes could be perscriptive for methods such as these.\n",
    "\n",
    "I also noticed my interests beginning to get much more refined around this time. I was really starting to hone in on multitask learning, multilingual learning, and non-convex loss landscapes as my predominant interests. I often had to force myself to read papers outside of these areas, since I would sometimes go for weeks without reading papers in other areas. Attending conferences ended up being the most effective way for me to break out of my bubble, and interact with research in other areas.\n",
    "\n",
    "### Summer\n",
    "\n",
    "\n",
    "During this summer my interests began to shift from multilingual learning to **multi-task learning in general**. I began to become a lot more interested in optimization and loss landscapes, and how these interact in a multi-objective setting. I also helped write a paper with Nick and Steven over ensemble distillation, which sparked my interest in **deep ensembles and model calibration**. This interest was further piqued at ICML 2020, when I attended a wonderful talk by [Andrew Wilson](https://cims.nyu.edu/~andrewgw/) over advances and challenges in Bayesian Model Averaging in deep learning.\n",
    "\n",
    "This summer saw the continuation of quarantine and social distancing. In the interest of full disclosure, I was starting to feel pretty bad about my progress as a PhD student. I'm not sure how much of this came from my lack of interaction with my peers, from not going outside as much or enjoying as many hobbies, and from how much time I spent on twitter - but I really began to feel as though I just wasn't as productive or motivated as I should be. I began to have doubts as to whether I was smart enough, or disciplined enough, to be a successful researcher. In happier news, I attended ACL and ICML 2020 online during this time, and had a very enjoyable conference experience, despite dearly missing the travel and in-person meetings.\n",
    "\n",
    "\n",
    "### Spring\n",
    "\n",
    "This semester I  wrapped up my first project with Mark and Nick, which was an ACL paper over **multilingual interference for NER models**. I was also taking 3 classes, which took up a substantial amount of my bandwidth. One of the classes I was taking was a **deep learning for dialogue** class, and I spent a decent amount of time reading and thinking about dialogue models and evaluations. I also worked on a small project with my peers and close friends [Mitchell](http://mitchgordon.me/) and [Elias](https://esteng.github.io/), which was geared towards building a novel dialogue evaluation metric based on semantic representations of common ground. However, the project didn't end up going much farther than a class presentation.\n",
    "\n",
    "This was also the semester that COVID-19 hit the U.S. - halfway through the semester classes moved to a remote setting, and the nation began to quarantine. I found it especially difficult to stay motivated during this time. While I was fortunate enough to have a decent work setup, I dearly missed the regular conversations with my labmates and the inspiration I derived from them. Getting most of my academic discourse from twitter only served to worsen my imposter syndrome and make me feel like I wasn't nearly as productive as I should have been. Of course, I'm extremely grateful to have had a stable source of funding and income, and a partner whom I lived with, who made quarantine a thousand times less isolating than it might have been."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019\n",
    "\n",
    "### Fall\n",
    "\n",
    "This semester I was a **TA for machine learning**! It's not really a research interest, but it took up a lot of research time, haha. Still this was an incredibly enriching experience, in which I learned a lot about writing homeworks, tests, and preparing lectures. It also taught me how much I love helping people understand things, and helped me to appreciate some of the fantastic professors I had during my undergraduate career. Finally, it really helped me to solidify my understanding of machine learning fundamentals.\n",
    "\n",
    "At this point, I was still continuing my **multilingual project** from my first semester. I continued being interested in multilingual training as a form of multi-task learning, but of the same task, and analyzing these models compared to monolingual models to see if we could find a way to explain their performance differences.\n",
    "\n",
    "I also had a passing interest in **neural tangent kernels, double descent and other fascinating findings** that were challenging our notion of overparameterization and overfitting with neural networks. I didn't pick up any projects regarding this, but I remained fascinated by the ongoing research around this subject.\n",
    "\n",
    "### Spring\n",
    "\n",
    "My interest in **multilingual models, especially multilingual BERT**, continued from the previous semester. I found it very interesting that sharing all parameters across all languages seemed to work so well, and found the cross-lingual abilities of multilingual BERT pretty fascinating. I spent some time working on a project which examined multilingual models on a smaller scale, to try to tease apart patterns of where this paradigm worked and didn't work well.\n",
    "\n",
    "Additionally, I was developing an interest in  **Neural Network Pruning and LTH**, mainly due to my friend [Mitch's](http://mitchgordon.me/) excitement about it and the discussions we had around it. I became very interested in sparsity of neural networks, and how that impacted my understanding of deep learning. I also spent a decent amount of time reading and thinking about **Bertology (probing)**, and what all of these probes were really telling us. I remember Mitch and I discussing the intersection of these to topics a lot, and at one point we were thinking about a project to prune BERT _then_ probe it to see what kinds of information it actually needed to perform the MLM task. He ended up writing a [paper](https://arxiv.org/abs/2002.08307) over a very similar idea later on, but it focused less on \"probing tasks\", and more on fine-tuning to NLU tasks.\n",
    "\n",
    "Finally, I had a brief period where I was really trying to understand **symbolic AI**, and it's relevance today. I think this came from my brief interest in grounding language models, which led to me reading The Symbol Grounding Problem, which then led me to realize that I had a very limited understanding of classic AI research and philosophy, so I spent some time reading and thinking about that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018\n",
    "\n",
    "### Fall\n",
    "\n",
    "**Starting my PhD!** I'm so fortunate that Mark Dredze was gracious enough to offer me a position in his lab as a PhD student, which began this semester. I also attended my first NLP conference ([EMNLP 2018](https://emnlp2018.org/)) this semester. My research interests were still bouncing around a lot, and I would become attached to anything that sounded cool for a couple weeks at a time, before dropping it for something else. Obviously, one thing that stayed on my radar around this time was BERT and related analysis papers, since it was becoming so predominant in the field.\n",
    "\n",
    "I also began to develop my first lasting research interest, which was in **multilingual models, and their behaviors**. My first project with Mark was to examine a particular multilingual model for NER and try to explain why it was able to do so well in a multilingual setting, when most other models fail to (this project ended up failing because I could never actually re-implement the model in question, and after 2 semesters of trying we shifted to an [analysis style paper](https://www.aclweb.org/anthology/2020.acl-main.720/)).\n",
    "\n",
    "### Spring\n",
    "\n",
    "Around this time, I was mainly interested in **information extraction tasks surrounding entities**, such as named entity recognition and entity linking. This was predominantly due to the fact that my course project with Greg involved entity linking and so I felt it was an area that I had a decent understanding of, so I was able to read relevant papers more thoroughly.\n",
    "\n",
    "Additionally, I would develop a sincere interest in **basically anything I read that sounded cool**, wherein my interest would fade a couple days later. I really had no idea what I was really interested in, and would read papers (like [ELMo](https://arxiv.org/abs/1802.05365)) without really understanding the overall impact it would eventually have on the field.\n",
    "\n",
    "At this time, I had just finished applying to graduate schools, so I spent most of my time worrying about my future and what I would do if I didn't get into any schools. Again, I am so, so thankful to Greg Durrett for devoting a significant amount of time to helping me apply for PhD programs, and his invaluable advice about programs, advisors, and SOP edits. It's fair to say I would not be anywhere near where I am today without Greg's early mentorship."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017\n",
    "\n",
    "### Fall\n",
    "\n",
    "**Understanding what NLP was**: In Fall of 2017 I started taking a [graduate course](https://www.cs.utexas.edu/~gdurrett/courses/fa2017/cs395t.shtml) taught by [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/) at UT. At this time I was working as a software developer in Austin, and I was beginning to suspect that I wanted to move to a more scientific career. I knew I had an interest in AI, and possibly AI research, but I wasn't sure how to start getting involved with the field. I had a lot of conversations with professors that left me feeling lost and a little hopeless, although two notable exceptions were professors [Alison Norman](https://www.cs.utexas.edu/~ans/) and [Bruce Porter](https://www.cs.utexas.edu/users/porter/), who instead gave me a lot of hope and encouragement. I will forever be grateful to Greg for allowing me to take his course, and to continue working on my course project with him after the class ended, which ended up going all the way to a conference submission.\n",
    "\n",
    "At this point in my career, I was just trying to understand the basics of machine learning and how it related to NLP. I was not really fluent in the literature of the field, and I couldn't really read many papers without feeling a little lost. I bought [Kevin Murphy's ML Book](https://probml.github.io/pml-book/book0.html) which I believe helped me understand fundamental ML concepts at a fairly deep level, and was essential to my success in Greg's class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
