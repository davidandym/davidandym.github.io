{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Draft - Gradient Covariance in Neural Networks\n",
    "\n",
    "Recently, my research has shifted towards studying optimization in deep learning, and in particular I've been interested in this notion of the _temperature_ of SGD, i.e. how much *noise* we incur in our estimation of the true gradient, given the gradient of only a single example, or perhaps a small batch.\n",
    "\n",
    "## Gradient Covariance in General\n",
    "\n",
    "### Formally, what _is_ gradient covariance?\n",
    "\n",
    "Let's start by defining a function, $f_\\theta$, and dataset, $\\mathcal{D}$.\n",
    "The function must be differentiable, and we'll assume that it's parameterized by $\\theta \\in \\mathbb{R}^d$.\n",
    "The dataset $\\mathcal{D}$ is composed of $N$ examples $x_1, x_2, \\ldots, x_N$, all of which may be passed into $f_\\theta(x)$. Finally, we will assume that we have some differentiable loss function $\\mathcal{L}(x, \\theta)$ which measures the error of $f_\\theta$ on example $x$. However, most of this will be abstracted away when we consider the gradient, which we'll term $G$ for simplicity.\n",
    "$$\n",
    "G(x) = \\nabla_\\theta \\mathcal{L}(f_\\theta(x), x)\n",
    "$$\n",
    "\n",
    "In short, $G(x)$ is the gradient of example $x$ given parameters $\\theta$. In optimization, we are often interested in the average gradient over the entire dataset, which we'll call the **true gradient** $G_T$\n",
    "$$\n",
    "G_T = \\frac{1}{N}\\sum_{x\\in\\mathcal{D}}G(x) \n",
    "$$\n",
    "The reason we care about the true gradient is because we often consider a _uniform distribution_ over our dataset, where each individual example $x_i$ in the dataset carries equal importance with respect to learning. There are exceptions to this, e.g. in curriculum learning where we determine some order under which a model should learn certain examples, but in most in practice cases we train models over the uniform distribution.\n",
    "\n",
    "One key issue with calculating $G_T$, especially in deep learning, is time - if your dataset is large (as they often are) it may take a very long time to compute $G(x)$ for every $x\\in\\mathcal{D}$.\n",
    "As a result, it is very common to instead _approximate_ $G_T$ by sampling a random example $x\\sim\\mathcal{D}$ and using $G(x)$ in place of $G_T$.\n",
    "In this way, we instead treat our gradient as a random variable $g$.\n",
    "This turns out to be a fairly reasonable approximation for a number of reasons - key amongst them is the fact that the **expected value** of $\\mathcal{G}$ over the entire dataset is $G_T$\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim\\mathcal{D}}[\\mathcal{G}] = \\frac{1}{N} \\sum_{x\\in\\mathcal{D}} G(x) = G_T\n",
    "$$\n",
    "so if we take a large number of steps, on average we will move in the right direction.\n",
    "However, the _noisiness_ of this estimation, i.e. how close each step will be to the average, is reflected in the **covariance matrix** of $\\mathcal{G}$,\n",
    "$$\n",
    "\\Sigma(\\mathcal{G}) = \\frac{1}{N}\\sum_{x\\in\\mathcal{D}}\\big[(G(x) - G_T)(G(x) - G_T)^T \\big]\n",
    "$$\n",
    "$\\Sigma(\\mathcal{G})$ is a $d\\times d$ matrix - the diagonals $\\Sigma(\\mathcal{G})_{i,i}$ represent the independent variance of the specific parameter $\\theta_i$, while $\\Sigma(\\mathcal{G})_{i,j}$ represents the _covariance_ between the two parameters $\\theta_i$ and $\\theta_j$. \n",
    "\n",
    "### _Intuitively,_ what _is_ gradient covariance?\n",
    "\n",
    "To give some very simple intuition about what's going on inside of these matrices, pictured above are 2 gradients, $G(x_1)$ and $G(x_2)$.\n",
    "\n",
    "### The Eigenvectors and Eigenvalues\n",
    "\n",
    "There are a couple of important facts to note about the Eigenvectors and Eigenvalues of a covariance matrix.\n",
    "\n",
    "The first, and most well-known, is that the eigenvectors point in the directions of highest variance while remaining orthogonal to each other.\n",
    "\n",
    "Another interesting fact is that the sum of the independent variances of each parameter $\\theta_i$ is actually equal to the sum of the eigenvalues of the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "One key application of the gradient is in the optimization of the loss function $\\mathcal{L}$ via gradient descent, which updates parameters $\\theta$ in the following way:\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^t - \\eta \\; G_T\n",
    "$$\n",
    "where $\\eta$ is the size of the \"step\" we take in the direction of $G_T$, and is a hyperparameter that we often have to tune by hand.\n",
    "As mentioned above, since $G_T$ is expensive to compute, we instead approximate it with a random sample from $\\mathcal{G}$... I'll write this as $G\\sim\\mathcal{G}$, although it's worth remembering that $G\\sim\\mathcal{G}$ is actually $G(x)$ computed for $x\\sim\\mathcal{D}$.\n",
    "This approximation yields the parameter update\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^t - \\eta \\; G\n",
    "$$\n",
    "and is called, of course, _stochastic gradient descent_, or SGD.\n",
    "The difference between SGD and standard gradient descent is that the step $\\eta\\;G$ has a bit of _noise_ in it - we're estimating $G_T$ with an approximation $G\\sim\\mathcal{G}$.\n",
    "This \"noise\" is characterized by the gradient covariance, which describes how close on average a random sample $G\\sim\\mathcal{G}$ will be to $G_T$.\n",
    "In practice SGD often converges slower than GD due to the noise in the gradient estimation. However, the tradeoff might be worth it if you only have to compute the gradient of a few examples at each step, rather than thousands.\n",
    "\n",
    "### The Step Size\n",
    "\n",
    "In optimization with SGD there is an additional notion of the \"noisy\"-ness of SGD, which refers to the noise in the step that SGD takes, e.g. how noisy is the step we are taking compared to a step in the \"true\" direction.\n",
    "In addition to the gradient covariance, there are two hyperparameters that can have a significant effect on the noise of the _step_, $\\eta \\;G$.\n",
    "\n",
    "We have already seen one - the step-size $\\eta$. $\\eta$ determines how large of a step we will take in the direction of $G\\sim\\mathcal{G}$.\n",
    "Obviously it is directly tied into the expectation of our step, via\n",
    "$$\n",
    "\\mathbb{E}_{G\\sim\\mathcal{G}}[\\eta \\; G] = \\eta \\; G_T\n",
    "$$\n",
    "and the norm of the step\n",
    "$$\n",
    "\\mathbb{E}\\big[ ||\\eta; G||^2 \\big] = \\eta\\; \\mathbb{E}\\big[ ||\\mathcal{G}||^2 \\big]\n",
    "$$\n",
    "However, it _also_ has an effect on the variance of $\\eta$ - when you scale a random variable by a fixed constant, it has a quadratic effect on the variance of that variable, i.e.\n",
    "$$\n",
    "\\Sigma(\\epsilon) = \\eta^2 \\; \\Sigma(\\mathcal{G})\n",
    "$$\n",
    "In almost every application of SGD that I have seen, $\\eta < 1$, and so the variance of the step of SGD is often smaller than the gradient covariance, just because we are scaling the step down.\n",
    "\n",
    "\n",
    "### The Effect of Batch-Size\n",
    "\n",
    "Often when using SGD for neural networks we do not actually sample only a single gradient $G\\sim\\mathcal{G}$. Instead, we consider a sample average $\\bar{\\mathcal{G}}_B$ over a sample of size $B$, i.e. $x \\sim \\{\\mathcal{D}\\}^B$.\n",
    "This averaging has an effect on the gradient covariance - the larger a sample we average over, the lower our variance will be. This variance reduction occurs linearly with respect to the sample size, so we can write the covariance of $\\bar{\\mathcal{G_B}}$ as\n",
    "$$\n",
    "\\Sigma(\\bar{\\mathcal{G_B}}) = \\frac{\\Sigma(\\mathcal{G})}{B}\n",
    "$$\n",
    "\n",
    "Note however that this linear scaling comes from assuming that each example $x_1, \\ldots, x_B\\sim\\mathcal{D}$ are sampled indepedently - in practice, we rarely actually sample indepedently because we sample without replacement, i.e. we don't want a batch to contain the same example more than once. Therefore, in theory, we have to correct our variance reduction with the FPC (finite population correction) factor\n",
    "$$\n",
    "\\Sigma(\\bar{\\mathcal{G_B}}) = \\frac{N - B}{N - 1}\\frac{\\Sigma(\\mathcal{G})}{B}\n",
    "$$\n",
    "\n",
    "However, if our batch size $B$ is significantly smaller than our dataset, i.e. $B \\ll N$, then the FPC is close to 1 - so in practice, we can usually assume that \n",
    "$$\n",
    "\\Sigma(\\bar{\\mathcal{G_B}}) \\approx \\frac{\\Sigma(\\mathcal{G})}{B}\n",
    "$$\n",
    "i.e. that the variance of our mini-batch gradient inversely scales with the size of the mini-batch.\n",
    "\n",
    "The batch-size also has an effect on the expected **norm of our gradient**, $\\mathbb{E}\\big[||\\mathcal{G}||^2\\big]$.\n",
    "Adding vectors together always results in a lower norm vector than the sum of the individual vector norms (see: triangle inequality), and so intuitively we expect that, on average, the gradient with a batch-size of 64 examples will have a smaller norm than a gradient with a batch-size of 16 examples.\n",
    "\n",
    "We can characterize the expected norm of $\\mathcal{G}$ in terms of the norm of $G_T$ and the _trace_ of the covariance as\n",
    "$$\n",
    "\\mathbb{E}\\big[ ||\\mathcal{G}||^2 \\big] = ||G_T||^2 + tr\\big(\\Sigma(\\mathcal{G})\\big)\n",
    "$$\n",
    "i.e. the expected norm of $\\mathcal{G}$ is the norm of the true gradient plus the _trace_ of the covariance matrix - note that the trace, $tr\\big(\\Sigma(\\mathcal{G})\\big)$, is the sum of the individual parameter's variances.\n",
    "We can see now how the batch-size affects the expected gradient norm - increasing the batch-size reduces the variance, resulting in a smaller second term\n",
    "$$\n",
    "\\mathbb{E}\\big[ ||\\bar{\\mathcal{G}_B}||^2 \\big] = ||G_T||^2 + \\frac{1}{B}tr\\big(\\Sigma(\\mathcal{G})\\big)\n",
    "$$\n",
    "Therefore, the larger our batch-size, the smaller our gradient norm will be on average. Of course, as well scale $B$ to be closer to $N$, the FPC factor comes back into play, which is how we get the second term to go to zero as $B \\rightarrow N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Temperature of SGD\n",
    "\n",
    "### The Linear Scaling Rule\n",
    "\n",
    "The above section suggests a simple rule when scaling the batch-size of SGD for a particular model - when we increase the batch-size by a factor of $k$, we should increase the learning rate by a factor of $\\sqrt{k}$. This scaling rule would allow us to keep the covariance of the gradient estimation constant as we increase the number of samples that we average over.\n",
    "\n",
    "Interestingly, in deep learning this is not true... the _linear scaling rule_ instead posits that when we increase the learning rate by a factor of $k$, we need to scale the learning rate by a factor of $k$ as well - i.e. rather than miantaining the gradient covariance constant, we should keep the ratio between the learning rate and batch-size constant.\n",
    "This ratio is often termed the _temperature_ of SGD\n",
    "$$\n",
    "T = \\frac{\\eta}{B}\n",
    "$$\n",
    "and there has been quite a bit of recent work that corroborates this finding.\n",
    "\n",
    "Why does the linear scaling rule hold in deep learning, and not the _square-root_ scaling rule? The answer to this is still unknown actually!\n",
    "\n",
    "\n",
    "### The SDE Approximation\n",
    "\n",
    "A stochastic differential equation is a function that takes the form of\n",
    "$$\n",
    "\\frac{\\delta x}{\\delta t} = \\nabla f(x) \\delta t + \\xi_{t + \\delta t}\n",
    "$$\n",
    "\n",
    "We can re-write SGD to look something like this by saying that the difference between $\\theta_{t+1}$ and $\\theta_t$ can be described as a step in the true gradient direction $\\eta\\;G_T$ plus the difference between the true gradient and our random sample, e.g.\n",
    "$$\n",
    "\\theta_{t+1} - \\theta_t = -\\eta \\; G_T + \\eta\\;(G_T - G)\n",
    "$$\n",
    "In order to derive an SDE from the above equation, we need $t$ to be a continuous variable - this is not exactly natural in SGD, since we take discrete steps.\n",
    "What we do to overcome this is we describe time in terms of intervals $\\Delta t$, where $\\Delta t$ is composed of several steps with a _very_ small, but fixed, learning rate $\\eta$, i.e. $\\Delta t = N\\;\\eta$.\n",
    "We then write our SGD update as\n",
    "$$\n",
    "\\theta_{t+\\delta t} - \\theta_t = - G_T\\delta t + \\sum_{n=1}^N \\eta\\;(G_T - G)\n",
    "$$\n",
    "Under this approximation we require a couple assumptions - the first is that $\\eta$ is _small_ enough so that, from step to step, the statistics of the gradient approximation (the second term) don't change that much. The second assumption is that $\\Delta t$ is _large_ enough that the law of large numbers applies to the second term - when the law of large numbers applies to the summation in the second term, we can replace it with a Gaussian random variable:\n",
    "$$\n",
    "\\theta_{t+\\Delta t} - \\theta_t = - G_T\\;\\Delta t + \\eta \\epsilon \\sqrt{N}\n",
    "$$\n",
    "where $\\epsilon \\sim \\mathcal{N}\\big(0, \\Sigma(\\mathcal{G})\\big)$. Note that $N = \\frac{\\Delta t}{\\eta}$, so we can replace $N$ and then multiply $\\eta$ _into_ that square-root to obtain\n",
    "$$\n",
    "\\theta_{t+\\Delta t} - \\theta_t = - G_T\\Delta t + \\epsilon \\sqrt{\\eta\\;\\Delta t}\n",
    "$$\n",
    "and we can finally move $\\sqrt{\\eta}$ into the _variance_ of $\\epsilon$, yielding:\n",
    "$$\n",
    "\\theta_{t+\\Delta t} - \\theta_t = - G_T\\Delta t + \\epsilon \\sqrt{\\Delta t}\n",
    "$$\n",
    "where $\\epsilon \\sim \\mathcal{N}\\big(0, \\Sigma(\\mathcal{G})\\big)$.\n",
    "We can re-write this in differentiable form:\n",
    "$$\n",
    "\\delta \\theta = - G_T \\delta t + \\epsilon \\sqrt{\\delta t}\n",
    "$$\n",
    "and, as is more common, we can move the variance of $\\epsilon$ back into the second term, and replace $\\sqrt{\\delta t}$ with $\\delta W_t$, where $W_t$ is a _Wiener process_ or _Brownian motion_, yielding the following SDE:\n",
    "$$\n",
    "\\delta \\theta = - G_T \\delta t + \\sqrt{\\eta \\Sigma(\\mathcal{G})} \\delta W_t\n",
    "$$\n",
    "\n",
    "Regardless of which version of the SDE we use, what is crucial is that the relationship between the gradient covariance $\\Sigma(\\mathcal{G})$ and the learning rate $\\eta$ is _linear_, e.g. increasing the learning rate by a factor of $k$ increases the noise of the SDE by a factor of $k$. Thus, when we increase the _batch-size_ to say $B$, our SDE becomes:\n",
    "$$\n",
    "\\delta \\theta = - G_T \\delta t + \\sqrt{\\frac{\\eta \\Sigma(\\mathcal{G})}{B}} \\delta W_t\n",
    "$$\n",
    "and finally, at long last, we have derived the theoretical motivation for the linear relationship between batch-size and learning rate, i.e. the _linear scaling rule_.\n",
    "\n",
    "For a little more intuition on this I strongly recommend watching David McAllester's Deep Foundation videos over [Gradient Flow](https://www.youtube.com/watch?v=oKbLeEL-Xro), [SDEs](https://www.youtube.com/watch?v=l4B_8DqYfmA), and [Temperature](https://www.youtube.com/watch?v=_Q6quuwjoRQ). They are very intuitive yet concise, and I found them really helpful when first approaching this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Covariance in Modern Neural Networks\n",
    "\n",
    "### Approximating the Covariance Matrix\n",
    "\n",
    "In deep learning, we can't compute the _whole_ covariance matrix - after all, it's a $d\\times d$ matrix where $d$ is often on the order of a million or higher, and the individual entries are floats - for simple reference, if $d = 1,000,000$ and we use 4 bytes to represent each entry, then the covariance matrix requires _4 Terabytes_ to store in memory all at once. Luckily, we rarely want to store the whole covariance matrix all at once. Instead we just want to calculate _properties_ of the covariance matrix, like it's trace or it's spectral norm.\n",
    "\n",
    "#### Trace\n",
    "\n",
    "The **trace** is a particularly nice property of the covariance matrix to compute - not only is it fairly simple to compute but it captures important properties of the overall matrix, namely the total variance of each parameter.\n",
    "It's actually not necessarily intractable to compute the trace _exactly_ - we just need to compute the gradient of each individual example, as well as the true gradient.\n",
    "$$\n",
    "tr\\big(\\Sigma(\\mathcal{G})\\big) = \\frac{1}{N} \\sum_{x \\in \\mathcal{D}} \\big(G(x) - G_T\\big)^T \\big(G(x) - G_T\\big)\n",
    "$$\n",
    "However, this can take _a lot of time_ to do depending on how big your dataset and model are. We can instead approximate it in a couple of ways.\n",
    "\n",
    "McCandlish et al., propose to estimate it by computing the gradient of a large batch size, $B_{large}$, which is composed of the gradients of several smaller batches, $B_{small}$.\n",
    "Recall from above that the batch-size has an effect on the expected gradient norm through the \n",
    "\n",
    "\n",
    "#### Spectral Norm\n",
    "\n",
    "### Empirical Observations of Gradient Covariance\n",
    "\n",
    "Several recent works have studied how gradient covariance evolves during training, and in particular how it's relation to the _temperature_ of SGD determines generalization.\n",
    "\n",
    "### Relation to the Hessian\n",
    "\n",
    "Finally, I'll note on an interesting relationship between the Hessian of a model and the Gradient Covariance Matrix.\n",
    "\n",
    "https://arxiv.org/pdf/1711.04623.pdf\n",
    "\n",
    "### Relation to the FIM\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1906.07774.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Footnotes\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#blog-post/GradientCovariance#cite_ref-1) Ahah... this is actually not something we should take for granted! In theory (and in practice for the most part) we operate on the IID assumption - that our examples are sampled. However, there is a whole subfield of machine learning, called curriculum learning, that challenges this assumption and generally actually yields pretty promising results (in my opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "<a name=\"reference-1\"></a>[1]\n",
    "\n",
    "[2]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
