{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My research interests over time.\n",
    "_Posted: 6/15/2019 Edited: 2/26/2021_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be interesting to retroactively examine the topics that interested me the most over the course of my PhD. Below are very broad paragraphs summarizing what was capturing my interest over a given semester (in reverse chronological order) starting from when I first started down the path to starting my PhD in the fall of 2017. The **bold** terms are the TL;DR for each paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020\n",
    "\n",
    "### Fall\n",
    "\n",
    "This semester my interest in multi-task learning began to join with my interest in ensembles and bayesian deep learning, resulting in a deep interest in the effects of **multi-task learning on loss landscapes**. This launched a project which has not yet culminated into anything substantial, but is certainly at the forefront of my interests. Additionally, I developed a strong interest in **deep multi-task learning optimization methods**, in particular those that modify the gradients of models, or methods that allow you to keep the hypothesis class constant between single-task and multi-task settings. A number of methods of this flavor were recently presented at ACL and Neurips 2020, and I wondered if my empirical exploration of loss-landscapes could be perscriptive for methods such as these.\n",
    "\n",
    "I also noticed my interests beginning to get much more refined around this time. I was really starting to hone in on multitask learning, multilingual learning, and non-convex loss landscapes as my predominant interests. I often had to force myself to read papers outside of these areas, since I would sometimes go for weeks without reading papers in other areas. Attending conferences ended up being the most effective way for me to break out of my bubble, and interact with research in other areas.\n",
    "\n",
    "### Summer\n",
    "\n",
    "Summer saw the continuation of quarantine and social distancing. In the interest of full disclosure, I was starting to feel pretty bad about my progress as a PhD student. I'm not sure how much of this came from my lack of interaction with my peers, from not going outside as much or enjoying as many hobbies, and from how much time I spent on twitter - but I really began to feel as though I just wasn't as productive or motivated as I should be. I began to have doubts as to whether I was smart enough, or disciplined enough, to be a successful researcher. In happier news, I attended ACL and ICML 2020 online during this time, and had a very enjoyable conference experience, despite dearly missing the travel and in-person meetings.\n",
    "\n",
    "Around this time, my interests began to shift from multilingual learning to **multi-task learning in general**. I began to become a lot more interested in optimization and loss landscapes, and how these interact in a multi-objective setting. I also helped write a paper with Nick and Steven over ensemble distillation, which sparked my interest in **deep ensembles and model calibration**. This interest was further piqued at ICML 2020, when I attended a wonderful talk by [Andrew Wilson](https://cims.nyu.edu/~andrewgw/) over advances and challenges in Bayesian Model Averaging in deep learning.\n",
    "\n",
    "### Spring\n",
    "\n",
    "This semester I  wrapped up my first project with Mark and Nick, which was an ACL paper over **multilingual interference for NER models**. I was also taking 3 classes, which took up a substantial amount of my bandwidth. One of the classes I was taking was a **deep learning for dialogue** class, and I spent a decent amount of time reading and thinking about dialogue models and evaluations. I also worked on a small project with my peers and close friends [Mitchell](http://mitchgordon.me/) and [Elias](https://esteng.github.io/), which was geared towards building a novel dialogue evaluation metric based on semantic representations of common ground. However, the project didn't end up going much farther than a class presentation.\n",
    "\n",
    "This was also the semester that COVID-19 hit the U.S. - halfway through the semester classes moved to a remote setting, and the nation began to quarantine. I found it especially difficult to stay motivated during this time. While I was fortunate enough to have a decent work setup, I dearly missed the regular conversations with my labmates and the inspiration I derived from them. Getting most of my academic discourse from twitter only served to worsen my imposter syndrome and make me feel like I wasn't near as productive as I should have been. Of course, I'm extremely grateful to have had a stable source of funding and income, and a partner whom I lived with, who made quarantine a thousand times less isolating than it might have been."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019\n",
    "\n",
    "### Fall\n",
    "\n",
    "This semester I was a **TA for machine learning**! It's not really a research interest, but it took up a lot of research time, haha. Still this was an incredibly enriching experience, in which I learned a lot about writing homeworks, tests, and preparing lectures. It also taught me how much I love helping people understand things, and helped me to appreciate some of the fantastic professors I had during my undergraduate career. Finally, it really helped me to solidify my understanding of machine learning fundamentals.\n",
    "\n",
    "At this point, I was still continuing my **multilingual project** from my first semester. I continued being interested in multilingual training as a form of multi-task learning, but of the same task, and analyzing these models compared to monolingual models to see if we could find a way to explain their performance differences.\n",
    "\n",
    "I also had a passing interest in **neural tangent kernels, double descent and other fascinating findings** that were challenging our notion of overparameterization and overfitting with neural networks. I didn't pick up any projects regarding this, but I remained fascinated by the ongoing research around this subject.\n",
    "\n",
    "### Spring\n",
    "\n",
    "My interest in **multilingual models, especially multilingual BERT**, continued from the previous semester. I found it very interesting that sharing all parameters across all languages seemed to work so well, and found the cross-lingual abilities of multilingual BERT pretty fascinating. I spent some time working on a project which examined multilingual models on a smaller scale, to try to tease apart patterns of where this paradigm worked and didn't work well.\n",
    "\n",
    "Additionally, I was developing an interest in  **Neural Network Pruning and LTH**, mainly due to my friend [Mitch's](http://mitchgordon.me/) excitement about it and the discussions we had around it. I became very interested in sparsity of neural networks, and how that impacted my understanding of deep learning. I also spent a decent amount of time reading and thinking about **Bertology (probing)**, and what all of these probes were really telling us. I remember Mitch and I discussing the intersection of these to topics a lot, and at one point we were thinking about a project to prune BERT _then_ probe it to see what kinds of information it actually needed to perform the MLM task. He ended up writing a [paper](https://arxiv.org/abs/2002.08307) over a very similar idea later on, but it focused less on \"probing tasks\", and more on fine-tuning to NLU tasks.\n",
    "\n",
    "Finally, I had a brief period where I was really trying to understand **symbolic AI**, and it's relevance today. I think this came from my brief interest in grounding language models, which led to me reading The Symbol Grounding Problem, which then led me to realize that I had a very limited understanding of classic AI research and philosophy, so I spent some time reading and thinking about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018\n",
    "\n",
    "### Fall\n",
    "\n",
    "**Starting my PhD!** I'm so fortunate that Mark Dredze was gracious enough to offer me a position in his lab as a PhD student, which began this semester. I also attended my first NLP conference ([EMNLP 2018](https://emnlp2018.org/)) this semester. My research interests were still bouncing around a lot, and I would become attached to anything that sounded cool for a couple weeks at a time, before dropping it for something else. Obviously, one thing that stayed on my radar around this time was BERT and related analysis papers, since it was becoming so predominant in the field.\n",
    "\n",
    "I also began to develop my first lasting research interest, which was in **multilingual models, and their behaviors**. My first project with Mark was to examine a particular multilingual model for NER and try to explain why it was able to do so well in a multilingual setting, when most other models fail to (this project ended up failing because I could never actually re-implement the model in question, and after 2 semesters of trying we shifted to an [analysis style paper](https://www.aclweb.org/anthology/2020.acl-main.720/)).\n",
    "\n",
    "### Spring\n",
    "\n",
    "Around this time, I was mainly interested in **information extraction tasks surrounding entities**, such as named entity recognition and entity linking. This was predominantly due to the fact that my course project with Greg involved entity linking and so I felt it was an area that I had a decent understanding of, so I was able to read relevant papers more thoroughly.\n",
    "\n",
    "Additionally, I would develop a sincere interest in **basically anything I read that sounded cool**, wherein my interest would fade a couple days later. I really had no idea what I was really interested in, and would read papers (like [ELMo](https://arxiv.org/abs/1802.05365)) without really understanding the overall impact it would eventually have on the field.\n",
    "\n",
    "At this time, I had just finished applying to graduate schools, so I spent most of my time worrying about my future and what I would do if I didn't get into any schools. Again, I am so, so thankful to Greg Durrett for devoting a significant amount of time to helping me apply for PhD programs, and his invaluable advice about programs, advisors, and SOP edits. It's fair to say I would not be anywhere near where I am today without Greg's early mentorship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017\n",
    "\n",
    "### Fall\n",
    "\n",
    "**Understanding what NLP was**: In Fall of 2017 I started taking a [graduate course](https://www.cs.utexas.edu/~gdurrett/courses/fa2017/cs395t.shtml) taught by [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/) at UT. At this time I was working as a software developer in Austin, and I was beginning to suspect that I wanted to move to a more scientific career. I knew I had an interest in AI, and possibly AI research, but I wasn't sure how to start getting involved with the field. I had a lot of conversations with professors that left me feeling lost and a little hopeless, although two notable exceptions were professors [Alison Norman](https://www.cs.utexas.edu/~ans/) and [Bruce Porter](https://www.cs.utexas.edu/users/porter/), who instead gave me a lot of hope and encouragement. I will forever be grateful to Greg for allowing me to take his course, and to continue working on my course project with him after the class ended, which ended up going all the way to a conference submission.\n",
    "\n",
    "At this point in my career, I was just trying to understand the basics of machine learning and how it related to NLP. I was not really fluent in the literature of the field, and I couldn't really read many papers without feeling a little lost. I bought [Kevin Murphy's ML Book](https://probml.github.io/pml-book/book0.html) which I believe helped me understand fundamental ML concepts at a fairly deep level, and was essential to my success in Greg's class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
