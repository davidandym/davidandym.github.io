{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My research interests over time.\n",
    "_Posted: 6/15/2019 Edited: 10/9/2022_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it might be interesting to retroactively examine the topics that interested me the most over the course of my PhD. Below are very broad paragraphs summarizing what was capturing my interest over a given semester (in reverse chronological order) along with some commentary starting from when I first started down the path to starting my PhD in the fall of 2017. The **bold** terms are the TL;DR for each paragraph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023\n",
    "\n",
    "### Summer\n",
    "\n",
    "This summer I did my first research internship at Netflix, in the Bay Area.\n",
    "This was a pretty exciting and fun experience, it was really nice to I think \n",
    "\n",
    "### Spring\n",
    "\n",
    "This spring semester was a pretty frustrating experience, to be completely honest. I felt like I was actually working _really_ hard this semester and yet, when all was said and done, I really didn't have a lot to show for it.\n",
    "I, once again, failed to finish up a paper in time for the proposed deadlines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022\n",
    "\n",
    "### Fall\n",
    "\n",
    "This fall was relatively exciting, because I got to attend NeurIPS. NeurIPS was a pretty interesting experience, it was easily the biggest conference I had ever been to, and simultaneously the most lonely? It was a somewhat frustrating experience because I felt extremely excited about a lot of the work being presented there, but I didn't really feel like a part of the community there; I didn't really have a group of people that I knew, or who knew me.\n",
    "I was also supposed to attend EMNLP in Abu Dhabi, where I knew way more people going, but sadly I couldn't fly because I ran into the 6-month passport expiration date issue, which I hadn't been aware of at the time (oops!).\n",
    "I was to present something at both conferences; at EMNLP I was presenting the work that I had finished over the summer, which was about the \n",
    "\n",
    "### Summer\n",
    "\n",
    "This summer passed by very quickly for me, but I accomplished 2 key things!\n",
    "We ran a paper from start to finish in a couple months and submitted it to the upcoming EMNLP. The paper was a move towards an earlier question I'd been ruminating on, which was about **how task-specification might affect multi-task conflict**.\n",
    "This paper was a first step towards this question, focusing on how the switch from \"shared-encoder, task-specific classifiers\" to \"shared encoder-decoder with task prompts\" affected things like multi-task conflict and negative transfer in multi-task settings.\n",
    "I also (finally) attempted my GBO, which was titled **\"The Role of Conflict in Multi-Task Learning\"**. I unconditionally passed, which means I got the go-ahead to start working on the different areas I identified as chapters of my eventual thesis.\n",
    "\n",
    "I also attended ICML this summer, and I wrote a blog post about it! It was my first time attending an in-person conference since Fall of 2018, and it was so great to finally be meeting people and talking about research face-to-face.\n",
    "\n",
    "### Spring\n",
    "\n",
    "Since moving, my mental health has been doing a lot better. I think this is likely less about NYC itself, and more about the fact that things were opening up again and I was starting to interact with people in person on a more regular basis... New York is very fun, though.\n",
    "The one thing that remained hard this semester was the distance between me and my labmates.\n",
    "I've met a lot of great people here in NYC but few of them really work on similar topics to me; I was definitely missing daily conversations about specific research ideas and dunking on the latest trends in the field (although few of my labmates actually work on similar topics as me, anyways).\n",
    "Other than that I was feeling really good about my PhD; I felt excited about the direction my research was moving in and was generally pretty motivated.\n",
    "\n",
    "Most of this semester was spent working on the paper I started last semester; namely, we had noticed a consistent trend with respect to the optimal learning rate for a multi-task model as the number of tasks increases.\n",
    "We connected this to the **temperature of SGD & generalization**, which launched an investigation into how multi-task objectives can effect gradient noise, and how that can yield negative transfer. An earlier version of this paper got rejected from AISTATS last semester, and I think it got rejected from ICML this semester. However, I remained pretty confident that the work was saying some interesting stuff for multi-task learning, so I wasn't giving up on this analysis yet. Additionally, I kept playing around with some ideas surrounding how we could show that **multi-task learning can mitigate underspecification**. However, this started with experiments around NLI and a random assortment of auxilliary tasks, with very mixed results. I think this direction needs a more principled approach, which is what I'll turn to next semester."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021\n",
    "\n",
    "### Fall\n",
    "\n",
    "At the beginning of this semester I moved with my partner to New York City and I started to rent out a coworking space. I was finally no longer working from home and it had a serious impact on my productivity; all of a sudden I was getting up early, sticking to a consistent schedule, feeling motivated and excited about research. Because of the vaccine mandate in NYC, I started to feel safer about going to the gym, eating at restaurants, etc. and that had such a drastic change on my mentality as well. It was almost like there had been this very low, confining cieling over my emotions, keeping me from feeling much more than a baseline level of excitement, and it was suddenly just gone. Of course, as I write this things are beginning to close back down because of the new variant, omicron, sweeping through the city - but I have hope that this new variant will pass quickly, and things will return to normal soon. I hope these aren't some \"famous last words\"...\n",
    "\n",
    "Research wise I was feeling good about things - For the first time since early 2020 I finished a paper! As of the writing of this section it hasn't been published, but I like the paper quite a bit and I'm excited about it's direction. It is broadly focused on some **empirical properties of optimization and multi-task learning**, and in particular **patterns that arise as the number of tasks changes**. I had also begun a project that was interested in examining how the **way in which we specify tasks affects inter-task conflict**, which was a paper more focused on the NLP aspect of multi-task learning and the rising trend of prompt-based task-specification. Finally, I was still interested in how **multi-task learning affects solution diversity** - However, I had realized that the answer to this question was not so straight forward, and depended on certain optimization properties that led to the first paper I discussed above.\n",
    "\n",
    "### Spring\n",
    "\n",
    "Oh boy... this semester was particularly rough for me. My productivity felt almost non-existent. I failed to reach 3 (I think?) conference deadlines that I set as goals, for various reasons (either not getting the results I wanted, or getting sudden doubts about the overall idea). I began to really feel the weight of having published only one first-author paper since starting my PhD, and a lot of doubt as to whether or not I could actually do good research began to creep in.\n",
    "Additionally, the total lack of regular communication and technical discussions with my peers was killer, especially as I began to work my way into research areas that I was far from an expert in and, I felt, required way more rigor and technical ability than I possessed.\n",
    "I have to express a lot of thanks to Nick and Mark for constantly attempting to keep me sane and give me perspective, especially Nick who really helped me regain some belief in myself that I was capable of making important contributions with the work I was doing.\n",
    "\n",
    "What's fascinating about this is that this was also the time when I started some projects that I really believed in. Despite not managing to bring any projects to completion, I was studying problems that I felt could have a lasting impact on the field, and that I was excited to work on. For example, I spent a large amount of time looking at **ensemble diversity with respect to multi-task models**. I was interested in answering the question of whether or not multi-task learning could have a negative effect on ensembling because of it's potential to reduce diversity between solutions. Related to this, I was also working on examining whether or not we could use **multi-task learning to mitigate underspecification in NLP**, to find models that were more reliably robust and agreed with humans on more challenging language understanding tasks.\n",
    "Finally, I was also attempting to quantify some **local properties of multi-task loss landscapes**. In particular, I was wondering if we could see consistent behaviors regarding how the local minima of multi-task loss landscapes related to nearby local minima of the single-task loss landscapes, and if there were any quantifiable differences between those solutions and solutions found using only a single-task objective.\n",
    "I think these are all super interesting directions and I was excited to work on all of them, but I don't feel like my productivity was able to match my excitement this semester. Additionally, as my work began to become more focused on analysis in ML, I constantly doubted whether I was being rigorous enough in my research to make real, lasting contributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020\n",
    "\n",
    "### Fall\n",
    "\n",
    "This semester my interest in multi-task learning began to join with my interest in ensembles and bayesian deep learning, resulting in a deep interest in the effects of **multi-task learning on loss landscapes**. This launched a project which has not yet culminated into anything substantial, but is certainly at the forefront of my interests. Additionally, I developed a strong interest in **deep multi-task learning optimization methods**, in particular those that modify the gradients of models, or methods that allow you to keep the hypothesis class constant between single-task and multi-task settings. A number of methods of this flavor were recently presented at ACL and Neurips 2020, and I wondered if my empirical exploration of loss-landscapes could be perscriptive for methods such as these.\n",
    "\n",
    "I also noticed my interests beginning to get much more refined around this time. I was really starting to hone in on multitask learning, multilingual learning, and non-convex loss landscapes as my predominant interests. I often had to force myself to read papers outside of these areas, since I would sometimes go for weeks without reading papers in other areas. Attending conferences ended up being the most effective way for me to break out of my bubble, and interact with research in other areas.\n",
    "\n",
    "### Summer\n",
    "\n",
    "Summer saw the continuation of quarantine and social distancing. In the interest of full disclosure, I was starting to feel pretty bad about my progress as a PhD student. I'm not sure how much of this came from my lack of interaction with my peers, from not going outside as much or enjoying as many hobbies, and from how much time I spent on twitter - but I really began to feel as though I just wasn't as productive or motivated as I should be. I began to have doubts as to whether I was smart enough, or disciplined enough, to be a successful researcher. In happier news, I attended ACL and ICML 2020 online during this time, and had a very enjoyable conference experience, despite dearly missing the travel and in-person meetings.\n",
    "\n",
    "Around this time, my interests began to shift from multilingual learning to **multi-task learning in general**. I began to become a lot more interested in optimization and loss landscapes, and how these interact in a multi-objective setting. I also helped write a paper with Nick and Steven over ensemble distillation, which sparked my interest in **deep ensembles and model calibration**. This interest was further piqued at ICML 2020, when I attended a wonderful talk by [Andrew Wilson](https://cims.nyu.edu/~andrewgw/) over advances and challenges in Bayesian Model Averaging in deep learning.\n",
    "\n",
    "### Spring\n",
    "\n",
    "This semester I  wrapped up my first project with Mark and Nick, which was an ACL paper over **multilingual interference for NER models**. I was also taking 3 classes, which took up a substantial amount of my bandwidth. One of the classes I was taking was a **deep learning for dialogue** class, and I spent a decent amount of time reading and thinking about dialogue models and evaluations. I also worked on a small project with my peers and close friends [Mitchell](http://mitchgordon.me/) and [Elias](https://esteng.github.io/), which was geared towards building a novel dialogue evaluation metric based on semantic representations of common ground. However, the project didn't end up going much farther than a class presentation.\n",
    "\n",
    "This was also the semester that COVID-19 hit the U.S. - halfway through the semester classes moved to a remote setting, and the nation began to quarantine. I found it especially difficult to stay motivated during this time. While I was fortunate enough to have a decent work setup, I dearly missed the regular conversations with my labmates and the inspiration I derived from them. Getting most of my academic discourse from twitter only served to worsen my imposter syndrome and make me feel like I wasn't near as productive as I should have been. Of course, I'm extremely grateful to have had a stable source of funding and income, and a partner whom I lived with, who made quarantine a thousand times less isolating than it might have been."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019\n",
    "\n",
    "### Fall\n",
    "\n",
    "This semester I was a **TA for machine learning**! It's not really a research interest, but it took up a lot of research time, haha. Still this was an incredibly enriching experience, in which I learned a lot about writing homeworks, tests, and preparing lectures. It also taught me how much I love helping people understand things, and helped me to appreciate some of the fantastic professors I had during my undergraduate career. Finally, it really helped me to solidify my understanding of machine learning fundamentals.\n",
    "\n",
    "At this point, I was still continuing my **multilingual project** from my first semester. I continued being interested in multilingual training as a form of multi-task learning, but of the same task, and analyzing these models compared to monolingual models to see if we could find a way to explain their performance differences.\n",
    "\n",
    "I also had a passing interest in **neural tangent kernels, double descent and other fascinating findings** that were challenging our notion of overparameterization and overfitting with neural networks. I didn't pick up any projects regarding this, but I remained fascinated by the ongoing research around this subject.\n",
    "\n",
    "### Spring\n",
    "\n",
    "My interest in **multilingual models, especially multilingual BERT**, continued from the previous semester. I found it very interesting that sharing all parameters across all languages seemed to work so well, and found the cross-lingual abilities of multilingual BERT pretty fascinating. I spent some time working on a project which examined multilingual models on a smaller scale, to try to tease apart patterns of where this paradigm worked and didn't work well.\n",
    "\n",
    "Additionally, I was developing an interest in  **Neural Network Pruning and LTH**, mainly due to my friend [Mitch's](http://mitchgordon.me/) excitement about it and the discussions we had around it. I became very interested in sparsity of neural networks, and how that impacted my understanding of deep learning. I also spent a decent amount of time reading and thinking about **Bertology (probing)**, and what all of these probes were really telling us. I remember Mitch and I discussing the intersection of these to topics a lot, and at one point we were thinking about a project to prune BERT _then_ probe it to see what kinds of information it actually needed to perform the MLM task. He ended up writing a [paper](https://arxiv.org/abs/2002.08307) over a very similar idea later on, but it focused less on \"probing tasks\", and more on fine-tuning to NLU tasks.\n",
    "\n",
    "Finally, I had a brief period where I was really trying to understand **symbolic AI**, and it's relevance today. I think this came from my brief interest in grounding language models, which led to me reading The Symbol Grounding Problem, which then led me to realize that I had a very limited understanding of classic AI research and philosophy, so I spent some time reading and thinking about that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018\n",
    "\n",
    "### Fall\n",
    "\n",
    "**Starting my PhD!** I'm so fortunate that Mark Dredze was gracious enough to offer me a position in his lab as a PhD student, which began this semester. I also attended my first NLP conference ([EMNLP 2018](https://emnlp2018.org/)) this semester. My research interests were still bouncing around a lot, and I would become attached to anything that sounded cool for a couple weeks at a time, before dropping it for something else. Obviously, one thing that stayed on my radar around this time was BERT and related analysis papers, since it was becoming so predominant in the field.\n",
    "\n",
    "I also began to develop my first lasting research interest, which was in **multilingual models, and their behaviors**. My first project with Mark was to examine a particular multilingual model for NER and try to explain why it was able to do so well in a multilingual setting, when most other models fail to (this project ended up failing because I could never actually re-implement the model in question, and after 2 semesters of trying we shifted to an [analysis style paper](https://www.aclweb.org/anthology/2020.acl-main.720/)).\n",
    "\n",
    "### Spring\n",
    "\n",
    "Around this time, I was mainly interested in **information extraction tasks surrounding entities**, such as named entity recognition and entity linking. This was predominantly due to the fact that my course project with Greg involved entity linking and so I felt it was an area that I had a decent understanding of, so I was able to read relevant papers more thoroughly.\n",
    "\n",
    "Additionally, I would develop a sincere interest in **basically anything I read that sounded cool**, wherein my interest would fade a couple days later. I really had no idea what I was really interested in, and would read papers (like [ELMo](https://arxiv.org/abs/1802.05365)) without really understanding the overall impact it would eventually have on the field.\n",
    "\n",
    "At this time, I had just finished applying to graduate schools, so I spent most of my time worrying about my future and what I would do if I didn't get into any schools. Again, I am so, so thankful to Greg Durrett for devoting a significant amount of time to helping me apply for PhD programs, and his invaluable advice about programs, advisors, and SOP edits. It's fair to say I would not be anywhere near where I am today without Greg's early mentorship."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017\n",
    "\n",
    "### Fall\n",
    "\n",
    "**Understanding what NLP was**: In Fall of 2017 I started taking a [graduate course](https://www.cs.utexas.edu/~gdurrett/courses/fa2017/cs395t.shtml) taught by [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/) at UT. At this time I was working as a software developer in Austin, and I was beginning to suspect that I wanted to move to a more scientific career. I knew I had an interest in AI, and possibly AI research, but I wasn't sure how to start getting involved with the field. I had a lot of conversations with professors that left me feeling lost and a little hopeless, although two notable exceptions were professors [Alison Norman](https://www.cs.utexas.edu/~ans/) and [Bruce Porter](https://www.cs.utexas.edu/users/porter/), who instead gave me a lot of hope and encouragement. I will forever be grateful to Greg for allowing me to take his course, and to continue working on my course project with him after the class ended, which ended up going all the way to a conference submission.\n",
    "\n",
    "At this point in my career, I was just trying to understand the basics of machine learning and how it related to NLP. I was not really fluent in the literature of the field, and I couldn't really read many papers without feeling a little lost. I bought [Kevin Murphy's ML Book](https://probml.github.io/pml-book/book0.html) which I believe helped me understand fundamental ML concepts at a fairly deep level, and was essential to my success in Greg's class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
